[{"content":"Introduction Direct Preference Optimization (DPO) is a method used to fine-tune large language models by directly optimizing their parameters based on human feedback. DPO effectively addresses the challenges associated with supervised fine-tuning during the advanced stages of reinforcement learning. The concept behind DPO is to integrate preference data directly into the model’s training process, adjusting the model’s parameters to increase the likelihood of generating outputs similar to those preferred by humans. In advanced stages of reinforcement learning, as the complexity of training escalates and the distinction between selected and discarded responses diminishes, Supervised Fine-Tuning (SFT) may inadvertently increase the rewards for incorrect responses.\nBackground In the field of Natural Language Processing (NLP), the introduction of InstructGPT 1 has significantly advanced research in reinforcement learning from human feedback (RLHF). Typically, the RLHF stage follows supervised fine-tuning (SFT). InstructGPT employs proximal policy optimization (PPO) for preference optimization, but PPO requires the training of an additional reward model. This reward model’s training is inherently unstable, and its outcomes directly impact the final optimization’s effectiveness. Subsequent research has bypassed the reward model training step, directly proceeding with preference training. Knowledge Training Optimization (KTO) 2 learns from non-paired preference data, while Initial Public Offering (IPO) 3 uses a root-finding MSE loss to incorporate Kullback-Leibler (KL) regularization. Order Police (ORPO) 4 introduces a reference-model-free odds ratio term to directly contrast winning and losing responses with the policy model, jointly training with the SFT objective.\nTheoretical foundation Bradley-Terry model The DPO algorithm 5, and true to its name, is to integrate the decision and reward functions directly into the reinforcement learning objective, eliminating the need for separate reward modeling. Moreover, DPO has emerged as a promising alternative for aligning Large Language Models (LLMs) to human or AI preferences.\nImagine that the large language model is prompted with prompts $x$ to produce pairs of answers, we are definitely going to prefer one over the other. Here, the Bradley-Terry model 6 is used to quantify preferences. This model is particularly used in a pairwise comparison, allowing us to express human preference distribution, denoted by $p^{*}$. Like most alignment methods, DPO requires a dataset of paired preferences $\\mathcal{D} = \\lbrace x^{(i)}, y_{w}^{(i)}, y_{l}^{(i)} \\rbrace_{i = 1}^{N}$ sampled from $p^{*}$, where $y_{w}$ and $y_{l}$ denotes the preferred and dispreferred choice. And we assume that the preferences are generated by an underlying reward model $r^{*}(y, x)$, which could be parametrized by $r_{\\phi}(y, x)$. Specifically, the probability that item $y_{w}$ is preferred over item $y_{l}$ could be modeled as\n$$ \\begin{equation} p^{*}(y_{w} \\succ y_{l} \\mid x) = \\frac{\\exp(r^{*}(x,y_{w}))}{\\exp(r^{*}(x,y_{w}))+\\exp(r^{*}(x,y_{l}))}. \\label{eq1} \\end{equation} $$\nIn Eq. \\eqref{eq1}, we still have a question, why do we use the exponential function?\nBy using the exponential function, we could transform ratios into a log-linear model, which is more convenient for optimization and computation. And then, this exponential function has a natural probabilistic interpretation in distribution models. Finally, the non-negativity of the exponential function ensures that model parameters are always positive, avoiding issues with negative scoring.\nThere is a more general explanation we could use as a reference. Assuming we do not use $\\exp(\\cdot)$ and replace it with $a^{x}$, we could get\n$$ \\begin{equation} p^{*}(y_{w}\\succ y_{l}) = \\frac{a^{(r^{*}(x,y_{w}))}}{a^{(r^{*}(x,y_{w}))}+a^{(r^{*}(x,y_{l}))}} \\end{equation} $$\nBut if we use this linear transformation, $b(x,y) = \\log_{a}(\\mathrm{e})c(x,y)$, so that\n$$ \\begin{equation} a^{b(x,y)} = a^{\\log_{a}(\\mathrm{e})c(x,y)} = (\\mathrm{e}^{\\log a})^{\\log_{a}(\\mathrm{e})c(x,y)} = \\mathrm{e}^{c(x,y)} \\end{equation} $$\nIt is not hard to find that for any base number $a$, we can still find a corresponding linear transformation, which brings us back to the exponential function. And it has those good properties that we mentioned earlier, which is why we chose it.\nLog-likelihood loss function We now have Eq. \\eqref{eq1}, taking the natural logarithm of both sides of the equation allows us to linearize the model\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}_ {R}(r_{\\phi}, \\mathcal{D}) \u0026 =-\\mathbb{E}_ {\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\frac{\\exp \\left(r_{\\phi}\\left(x, y_w\\right)\\right)}{\\exp \\left(r_{\\phi}\\left(x, y_w\\right)\\right)+\\exp \\left(r_{\\phi}\\left(x, y_l\\right)\\right)}\\right] \\\\ \u0026 =-\\mathbb{E}_ {\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\frac{1}{1+\\exp \\left(r_{\\phi}\\left(x, y_l\\right)-r_{\\phi}\\left(x, y_w\\right)\\right)}\\right] \\\\ \u0026 =-\\mathbb{E}_ {\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(r_{\\phi}\\left(x, y_w\\right)-r_{\\phi}\\left(x, y_l\\right)\\right)\\right] \\end{aligned} \\label{eq4} \\end{equation} $$ where $\\sigma$ is the logistic function, the more specific expression is $ \\sigma (x)= {1}/({1 +\\exp{(-x)}})$.\nDuring the reinforcement learning fine-tuning phase, we need to find the following target to provide feedback to the language model,\n$$ \\begin{equation} \\max_{\\pi_{\\theta}} \\mathbb{E}_ {x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}}[r_{\\phi}(x,y)] - \\beta \\mathbb{D}_ {\\text{KL}}[\\pi_{\\theta}(y\\mid x) || \\pi_{\\text{ref}}(y\\mid x)] \\label{eq5} \\end{equation} $$\nwhere $\\beta$ is a hyper-parameter the controls the deviation from baseline reference policy $\\pi_{\\text{ref}}$, and $\\pi_{\\theta}$ is the language model policy. For this equation, $\\mathbb{E}_ {x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}}[r_ {\\phi}(x,y)]$, reflects our desire to achieve as much reward as possible in the given task while simultaneously minimizing $\\mathbb{D}_ {\\text{KL}}[\\pi_{\\theta}(y\\mid x) || \\pi_{\\text{ref}}(y\\mid x)]$, which measures how close or far the model distribution is from the reference distribution.\nDeriving the optimum of reward maximization objective For two probability distributions $P$ and $Q$, Kullback-Leibler (KL) divergence $\\mathbb{D}_{\\mathrm{KL}}(P|| Q)$ is defined as\n$$ \\begin{equation} \\mathbb{D}_ {\\text{KL}}(P || Q)= \\sum_{x \\in \\mathcal{X}} P(x)\\log \\left( \\frac{P(x)}{Q(x)} \\right) \\end{equation} $$\nwhere $\\mathcal{X}$ is the set of possible outcomes for the discrete random variable $x$. KL divergence measures the information loss when distribution $Q$ is used to approximate distribution $P$. In other words, it quantifies the difference between distributions $Q$ and $P$. In the following derivation, we need to introduce the concept of partition function in physics, $Z (x)$. With Eq. \\eqref{eq5}, we could have\n$$ \\begin{equation} \\begin{aligned} \u0026 \\max_ {\\pi_{\\theta}} \\mathbb{E}_ {x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}} [r_{\\phi}(x, y)] -\\beta \\mathbb{D}_ {\\mathrm{KL}}\\left[\\pi_{\\theta}(y \\mid x) \\| \\pi_{\\mathrm{ref}}(y \\mid x)\\right] \\\\ \u0026 = \\max_{\\pi_{\\theta}} \\mathbb{E}_ {x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}}[r_{\\phi}(x, y)] - \\mathbb{E} _ {x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}}\\left[ \\beta \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} \\right]\\\\ \u0026 =\\max_ {\\pi_{\\theta}} \\mathbb{E}_ {x \\sim \\mathcal{D}} \\mathbb{E}_ {y \\sim \\pi_{\\theta}}\\left[r_{\\phi}(x, y)-\\beta \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)}\\right] \\\\ \u0026 =\\min_ {\\pi_{\\theta}} \\mathbb{E}_ {x \\sim \\mathcal{D}} \\mathbb{E}_ {y \\sim \\pi_{\\theta}}\\left[\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)}-\\frac{1}{\\beta} r_{\\phi}(x, y)\\right] \\\\ \u0026 =\\min_ {\\pi_{\\theta}} \\mathbb{E}_ {x \\sim \\mathcal{D}} \\mathbb{E}_ {y \\sim \\pi_{\\theta}}\\left[\\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\color{red}{\\frac{1}{Z(x)} \\pi_{\\mathrm{ref}}(y \\mid x) \\exp \\left(\\frac{1}{\\beta} r_{\\phi}(x, y)\\right)}}-\\log Z(x)\\right], \\end{aligned}\\label{eq7} \\end{equation} $$ where partition function is\n$$ \\begin{equation} Z (x) = \\sum_{y}\\pi_{\\text{ref}(y \\mid x) }\\exp\\left( \\frac{1}{\\beta} r_{\\phi} (x, y) \\right). \\end{equation} $$\nLet us go back to statitical physics for explaining the partition function, partition function $Z$ is the sum of all possible state weights of a system, and it determines the equilibrium properties of this system. For a given system, the partition function in physics is defined as $ Z = \\sum_{i} \\exp\\left(-\\frac{E_{i}}{k_{B}T}\\right)$, where $E_{i}$ is the energy of the $i$-th state, $k_{B}$ is the Boltzmann constant, and $T$ is the temperature of the system. Physically, the partition function $Z$ represents a normalization factor in the system\u0026rsquo;s state space that determines the probability of each state. In the given optimization framework, our goal is to obtain the policy $\\pi_\\theta$ by maximizing the reward $r_\\phi(x, y)$ while minimizing the KL divergence between the distribution $\\pi_\\theta(y \\mid x)$ and the reference distribution $\\pi_{\\text{ref}}(y \\mid x)$. To simplify the computation of the KL divergence, we could introduce a partition function $Z(x)$, which is similar to the partition function in statistical physics. This function normalizes the weighted probabilities across all possible values of $y$. And then we need to define\n$$ \\begin{equation} \\begin{aligned} \\frac{1}{Z(x)} \\pi_{\\mathrm{ref}}(y \\mid x) \u0026\\exp \\left(\\frac{1}{\\beta} r_{\\phi}(x, y)\\right) \\\\ \u0026 = \\frac{\\pi_{\\text{ref}}(y \\mid x)\\exp \\left( \\frac{1}{\\beta}r_{\\phi}(x, y) \\right)}{\\sum_{y}\\pi_{\\text{ref}(y \\mid x)}\\exp\\left( \\frac{1}{\\beta}r_{\\phi}(x,y) \\right)}\\\\ \u0026 = \\pi^{*}(y \\mid x) \\end{aligned} \\end{equation} $$ Now we should re-organize the detailed result in Eq. \\eqref{eq7} as\n$$ \\begin{equation} \\begin{aligned} \\min_{\\pi_{\\theta}}\\mathbb{E}_ {x \\sim \\mathcal{D}, y \\sim \\pi_{ \\theta}}\u0026 \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi^{*}(y \\mid x)} - \\log Z(x) \\right] \\\\ \u0026 = \\min_{\\pi_{\\theta}}\\mathbb{E}_ {x \\sim \\mathcal{D}, y \\sim \\pi_{ \\theta}} \\left[ \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi^{*}(y \\mid x)} \\right] \\\\ \u0026 = \\min_{\\pi_{\\theta}}\\mathbb{E} _ {x \\sim \\mathcal{D}}[\\mathbb{D}_ {\\text{KL}}(\\pi_{\\theta}(y \\mid x) ||\\pi^{*}(y \\mid x))]. \\end{aligned} \\end{equation} $$ Since we all know $Z(x)$ does depend on $y$, only if the two distributions are identical we will find that the KL-divergence is minimized at $0$. Hence we got the corresponding optimal solution\n$$ \\begin{equation} \\pi_{\\theta}(y \\mid x)=\\pi^*(y \\mid x)=\\frac{1}{Z(x)} \\pi_{\\mathrm{ref}}(y \\mid x) \\exp \\left(\\frac{1}{\\beta} r_{\\phi}(x, y)\\right).\\label{eq11} \\end{equation} $$\nAnd then we derive $r_{\\phi}$ from Eq. \\eqref{eq11}\n$$ \\begin{equation} \\begin{aligned} r_{\\phi}(x, y) \u0026=\\beta \\log \\left[\\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} Z(x)\\right] \\\\ \u0026 =\\beta \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}+\\beta \\log Z(x). \\end{aligned} \\label{eq12} \\end{equation} $$ By substituting Eq. \\eqref{eq12} into Eq. \\eqref{eq4} we could obtain\n$$ \\begin{equation} \\begin{aligned} \u0026 \\mathcal{L}_ {\\mathrm{DPO}}\\left(\\pi_\\theta ; \\pi_{\\mathrm{ref}}\\right)= -\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(r_\\phi\\left(x, y_w\\right)-r_\\phi\\left(x, y_l\\right)\\right)\\right]\\\\ \u0026 = -\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_w \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_w \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_l \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_l \\mid x\\right)}\\right)\\right] \\end{aligned} \\end{equation} $$ Discussion Recent Literature Review. Even recently, researchers unveil SimPO 7, an innovative algorithm that streamlines reinforcement learning by leveraging human feedback. By adopting the average log probability of a sequence as an inherent reward, SimPO cleverly bypasses the complexity of using a reference model. This method not only simplifies the process but also proves to be more efficient in computation and memory usage. When pitted against established methods such as DPO, especially in benchmarks like AlpacaEval 2 and Arena-Hard, SimPO demonstrates superior performance.\nReferences L. Ouyang, J. Wu, X. Jiang, et al., “Training language models to follow instructions with human feedback,” Advances in neural information processing systems, vol. 35, pp. 27 730–27 744, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela, “Kto: Model alignment as prospect theoretic optimization,” arXiv preprint arXiv:2402.01306, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Jiang, X. Huang, and C. Wei, “Preference as reward, maximum preference optimization with importance sampling,” arXiv preprint arXiv:2312.16430, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Hong, N. Lee, and J. Thorne, “Reference-free monolithic preference optimization with odds ratio,” arXiv preprint arXiv:2403.07691, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, “Direct preference optimization: Your language model is secretly a reward model,” Advances in Neural Information Processing Systems, vol. 36, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. A. Bradley and M. E. Terry, “Rank analysis of incomplete block designs: I. the method of paired comparisons,” Biometrika, vol. 39, no. 3/4, pp. 324–345, 1952.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Meng, M. Xia, and D. Chen, “Simpo: Simple preference optimization with a reference-free reward,” arXiv preprint arXiv:2405.14734, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://kairoswang.github.io/posts/2024-06-24-dpo/","summary":"Introduction Direct Preference Optimization (DPO) is a method used to fine-tune large language models by directly optimizing their parameters based on human feedback. DPO effectively addresses the challenges associated with supervised fine-tuning during the advanced stages of reinforcement learning. The concept behind DPO is to integrate preference data directly into the model’s training process, adjusting the model’s parameters to increase the likelihood of generating outputs similar to those preferred by humans.","title":"Viewing Direct Policy Optimization from a Physical Perspective"},{"content":"Introduction Mathematics and science are deeply interconnected, with math providing the founfation for scientific theories. However, there exists an huge gap within mathematics—a gap that ensures we will never touch complete certainty. Certain true statements remain unprovable, potentially challenging our belief in scientific discoveries.\nHistorical context Russell\u0026rsquo;s paradox and formalism In 1874, Georg Cantor introduced the concept of sets, and a branch of mathematics named set theory, which studies well-defined collections of things. Despite his attempts to establish a rigorous mathematical framework based on set theory, Cantor acknowledged the inherent difficulties in defining limit precisely. By the end of the 19-th century, a huge debate among mathematicians centered around the foundations of mathematics. On the one hand were the intuitionists led by Luitzen Egbertus Jan Brouwer, who emphasized the constructive aspects of mathematics, arguing that mathematical truths are not discovered but created. On the other hand were the formalist championed by David Hilbert, they aimed to prove the consistency of mathematical systems through formal methods. But Bertrand Russell discovered a paradox in the late spring of 1901 while working within the framework of naive set theory. Russell\u0026rsquo;s paradox arises when considering the set $R$ of all sets that do not contain themselves. Is $R$ a member of itself?\nIf $R \\in R$, by definition of $R, R \\notin R$. If $R \\notin R$, by definition of $R, R \\in R$. Russell\u0026rsquo;s paradox exposed a fundamental contradiction in naive set theory, a logical contradiction comes from self-reference. One common approach to addressing this paradox involves refining how sets are defined. Please consider the formula $\\phi(x)$ as $x \\not\\in x$. The set $\\lbrace x\\in S: x \\not\\in x \\rbrace $ is not contradictory because it only includes elements in $ S $ that are not members of themselves 1, it avoids including itself within the set. However, this story continues.\nHilbert\u0026rsquo;s program Hilbert\u0026rsquo;s program, formulated by the German mathematician David Hilbert in the early 1920s, aimed to address the foundational crisis in mathematics by establishing a new system for mathematical proofs 2. The construction of a system of proof, relies on the assumption that certain axioms are true. For example, a straight line segment could be drawn joining any two points in Euclidean geometry. And two sets are equal if and only if they contain the same elements in set theory. These axioms serve as basic building blocks for constructing mathematical systems. Theorems can be derived from axioms using a set of inference rules. It is a structured framework used to determine the validity of propositions or statements. At the International Congress of Mathematicians held in Bologna, Italy, in 1928 David Hilbert returned to the second one of the 23 problems, asking 3\nIs mathematics complete? Is mathematics consistent? Is mathematics decidable? Completeness means that every true mathematical statement could be proven by using an assumed formal system. And consistency means that the system does not lead to any contradictions; in other words, you can\u0026rsquo;t prove both a statement and its opposite. If the final one is true, it suggests that whether any given mathematical statement is true or false could be determinied. Hilbert was convinced that all the answers of those questions were yes, as he saying before, Wir müssen wissen, wir werden wissen (We must know, we will know).\nGödel\u0026rsquo;s incompleteness theorems Gödel\u0026rsquo;s incompleteness proof In 1931, Gödel addressed Hilbert\u0026rsquo;s first question with his incompleteness theorem 4. This theorem established that it is impossible to construct a formal system, using the axiomatic method, for any branch of mathematics containing arithmetic that encompasses all its truths. This meant that no matter how carefully mathematicians chose their axioms, there would always be some truths that escaped formal proof 5. Gödel\u0026rsquo;s incompleteness proof is achieved by assigning unique Gödel numbers to symbols and statements, constructing a self-referential statement that asserts its own unprovability and using logical contradictions to demonstrate that it must be true but unprovable. The key point is still the thing called self-reference. In 6, the author uses an analogy to illustrate this similar thought, if editor claims that every concept is defined in that dictionary, then it must be logically circular. This is like defining the first word using a second word, and then explaining the second word with a third, continuing indefinitely. And then Gödel\u0026rsquo;s Second Incompleteness Theorem shows that no consistent formal system includes basic arithmetic can prove its own consistency. These groundbreaking results revolutionized the fields of mathematics and logic, profoundly impacting the philosophy of mathematics.\nUndecidability in mathematics How about addressing the last question of decidability in mathematics? Interestingly, this question is actually related to Turing machine. Turing imagined an entirely mechanical computer, it consists of an infinite tape divided into cells, a head that can read and write symbols on the tape, and a set of rules that dictate the machine’s actions based on the current state and the symbol it reads. The decidability of a proposition may be considered equivalent to the famous Turing halting problem in computational theory 7. The most famous example of an undecidable problem is Halting Problem, which asks whether a given Turing machine will halt (stop running) or continue to run forever based on a given input. Alan Turing proved that there is no algorithm can solve the Halting Problem for all possible Turing machine and input pairs. Furthermore, the problem of undecidability even appears in complex physical systems. Cubitt et al. shows that determining whether a quantum system has a spectral gap (a gap in energy levels) is undecidable. This means that there is no algorithm could determine for all possible Hamiltonians whether they are gapped or gapless 8. Now, we could answer those questions from Hilbert, the truth is we could not know, we will not know.\nHow do we see science? The correctness of mathematics is equivalent to the correctness of its axioms. It is assumed in mathematics that axioms are self-evident truths from which all mathematical theories are derived. However, the incompleteness, inconsistency, and undecidability inherent in mathematics remind us that even the most fundamental axioms may have their limitations. These limitations could unexpectedly influence various fields of natural science in ways we may not anticipate. Regarding the question “How do we see science?” I have some further thinkings as follows.\nThe problem of induction Inductive reasoning, by its nature, lacks strict logical rigor; it serves as a summary of scientific empirical facts to make broader generalizations or predictions which could rise numerous issues. For instance, observing that the sun has risen every day in the past does not logically ensure that it will rise tomorrow. This is because such an inference assumes that the future will resemble the past, which is itself an unproven assumption. It leads to Hume problem articulated by the Scottish philosopher David Hume. Although we rely on inductive reasoning to navigate the world, there is no rational basis for the belief that the future will be consistent with the past. This presents a paradox, despite the lack of rational justification, inductive reasoning seems both indispensable and reliable in practice.\nBayesian approach to belief and evidence Employing the views of reinforcement learning, we could envision world as an enigmatic black box—a realm where our initial understanding is starkly devoid of knowledge. Through persistent trial and error, we receive feedback from environment, gradually deepening our comprehension of reality. For instance, in ancient times, as people pushed stones, they observed that exerting greater force resulted in faster motion. Through relentless experimentation, this insight was reinforced. In the virtual realm of computing, such logic manifests as foundational code, while in the tangible world, it is encapsulated by mathematics, physics, and the broader domain of science.\nThese perspectives resonate closely with Bayesian epistemology, prior probability represents the degree of belief in a hypothesis before new evidence is taken into account. As evidence is gathered, the prior is updated to the posterior probability, which reflects the revised belief in the hypothesis after considering the new evidence. We strive to approximate the natural laws, perpetually reaching for the profound mysteries of the universe. Nevertheless, the quintessential nature of these truths eludes us, shrouded in obscurity much like the form casts the shadow itself.\nChallenges and criticisms Even within the formalistic framework of logic, there persist certain doubts that challenge the very foundations upon which it stands. Someone has raised questions about law of excluded middle. The principle of excluded middle typically states that in any statement, it is either true or false, with no other possibilities. Is it possible for there to exist a proposition where we cannot determine its correctness or incorrectness, meaning that the law of excluded middle does not apply? Another one is about topos theory, it introduces a novel perspective on truth and logic. Unlike classical logic, where statements are simply true or false, topos theory accommodates a more nuanced view where the truth of a statement can vary depending on the context 9. This approach has implications for the foundations of scientific cognition, offering an alternative prespective to view science.\nDiscussion Science is not just a collection of facts, it is a dynamic path to understanding. Moreover, science is also about curiosity and the willingness to explore and ask questions about the world around us. It’s about being open to new ideas and evidence, and being ready to revise our understanding as new information becomes available. It is conceivable that we may never reach the ultimate shores of truth, our journey along the path towards enlightenment will persist unceasingly.\nReferences Irvine, A. D. \u0026amp; Deutsch, H. in The Stanford Encyclopedia of Philosophy (ed Zalta, E. N.) Spring 2021 (Metaphysics Research Lab, Stanford University, 2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZach, R. in The Stanford Encyclopedia of Philosophy (eds Zalta, E. N. \u0026amp; Nodelman, U.) Winter 2023 (Metaphysics Research Lab, Stanford University, 2023).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHilbert, D. Probleme der Grundlegung der Mathematik. I, 135–141 (1929).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGdel, K. Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I. Monatshefte für mathematik und physik 38, 173–198 (1931).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGödel, K. On formally undecidable propositions of Principia Mathematica and related systems (Courier Corporation, 1992).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCui, W. Can Science Reveal the Origin of the Universe? European Journal of Applied Sciences–Vol 12 (2024).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTuring, A. M. et al. On computable numbers, with an application to the Entscheidungsproblem. J. of Math 58, 5 (1936).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCubitt, T. S., Perez-Garcia, D. \u0026amp; Wolf, M. M. Undecidability of the spectral gap. Nature 528, 207–211 (2015).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJohnstone, P. T. Topos theory (Courier Corporation, 2014).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://kairoswang.github.io/posts/2024-06-01-belief_system/","summary":"Introduction Mathematics and science are deeply interconnected, with math providing the founfation for scientific theories. However, there exists an huge gap within mathematics—a gap that ensures we will never touch complete certainty. Certain true statements remain unprovable, potentially challenging our belief in scientific discoveries.\nHistorical context Russell\u0026rsquo;s paradox and formalism In 1874, Georg Cantor introduced the concept of sets, and a branch of mathematics named set theory, which studies well-defined collections of things.","title":"The challenges of scientific truth and belief systems"},{"content":"Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.\nDefinition of Deep Reinforcement Learning Something about definitions \u0026amp; notation Here, I recommend this textbook named Mathematical foundation of reinforcement learning for studying the fundamental concepts related to classical reinforcement learning.\n$\\mathbf{s}_{t}$ - state $\\mathbf{o}_{t}$ - observation $\\mathbf{a}_{t}$ - action $\\pi _{\\theta}({\\mathbf{a} _{t}} | \\mathbf{o} _{t})$ - policy (partially observed) - Instead, it receives an observation $\\mathbf{o} _{t}$, which might be a partial or noisy view of the true state. This policy defines the probability of agent taking action $\\mathbf{a} _{t}$ at​ the given observation. $\\pi _{\\theta}({\\mathbf{a} _{t}} | \\mathbf{s} _{t})$ - policy (fully observed) - In a fully observed environment, the agent has access to complete state $\\mathbf{s} _{t}$ of environment. The policy $\\pi _{\\theta}(\\mathbf{a} _t | \\mathbf{s} _t)$ defines the probability of agent taking action $\\mathbf{a}_t$. Among all of them, important definitions to know are the state which we denote $\\mathbf{s}_{t}$, the observation $\\mathbf{o} _{t}$ and the action $\\mathbf{a} _{t}$. Then, the observation and state could be related to one another by the following graphical model where the edge between observations and actions is policy, and state satisfies the Markov property.\nWe\u0026rsquo;ll start with something called Markov chain, which is named after Andrei Markov who was a mathematician pioneered the study of stochastic processes. The Markov chain has a very simple definition，it consists of just two things, a set of states s and a transition function, which means that the state at time $t+1$ is independent of the state at time $t-1$, one condition on the current state $\\mathbf{s}_{t}$.\nMarkov chain:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{T}}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{T}$ - transition operator $p(s_{t+1}|s_{t})$, a sort of linear operator, it can also be referred to as a transition probability or a dynamics function. It sounds like a little weird, why is it refferred to an operator?\nAnswer: This operator emphasizes how the Markov chain\u0026rsquo;s dynamics are governed: it takes the current distribution of states and produces the next distribution, much like how a function or matrix transforms inputs to outputs in mathematics. It will help us capture the essence of how states evolve over time in the Markov process.\nIf we represent the probabilities of each state at time step $t$ as a vector, we could call it $\\mu_{t,i}=p(s_{t}=i)$. Let\u0026rsquo;s say we have $n$ states, each with its own probability distribution represented by a probability vector $\\vec{\\mu _{t}}$, where $t$ represents the time step. Then, the transition probabilities as a matrix, where the $ij$-th entry is the probability of going into state $i$ if you are currently in the state $j$, the corresponding formula is $\\mathcal{T} _{i,j}= p(s _{t+1} = i | s _{t}=j)$. Now, we could express the vector of state probabilities at the next time step $\\vec{\\mu _{t+1}} = \\mathcal{T}\\vec{\\mu _{t}}$.\nHowever, the Markov chain itself doesn\u0026rsquo;t allow us to specify a decision, as it lacks the notion of actions. In order to go towards the notion of actions, we have to turn the Markov chain into a Markov decision process (MDP).\nMarkov decision process:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, r}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{A}$ - action space, actions $a \\in \\mathcal{A}$ (discrete or continuous) $\\mathcal{T}$ - transition operator, it\u0026rsquo;s not a matrix any more, but a tensor! Because it includes next state, current state, and current action. $r$ - reward function, $r: \\mathcal{S}\\times \\mathcal{A} = \\lbrace (s,a)| s \\in \\mathcal{S}, a \\in \\mathcal{A} \\rbrace \\rightarrow \\mathbb{R}$, the reward function is a mapping from the Cartesian product of the state and action space into real valued numbers. Some useful tricks we mentioned earlier can still be applied here, let $\\mu_{t,j} = p(s_{t}=j)$, and we could have another vector $\\xi_{t,k}=p(a_{t}=k)$ that will denote the probability of taking some action. Also, transition operator could be written as a tensor, so $\\mathcal{T} _{i,j,k}= p(s _{t+1}=i | s _{t}=j,a _{t}=k)$ is the probability of entering state $i$ if you\u0026rsquo;re in state $j$ and take action $k$. The vector of state probabilities at the next time step will be a little bit complex,\n$$ \\mu_{t+1,i}=\\sum_{j,k}\\mathcal{T}_{i,j,k}\\mu_{t,j}\\xi_{t,k} $$ (P.S. the reason why we don't use $\\vec{}$ is that $\\mu_{t+1,i}$ as a tensor, not matrix.) Before we go to the next part, we\u0026rsquo;d like to extend this Markov decision process definition, which will allow us to bring in the notion of observations. So a partially observed Markov decision process further augments the definition within two additional objects, observation space $\\mathcal{O}$ and emission probability $\\mathcal{E}$.\nPartially observed Markov decision process:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{O}, \\mathcal{T}, \\mathcal{E}, r}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{A}$ - action space, actions $a \\in \\mathcal{A}$ (discrete or continuous) $\\mathcal{O}$ - observation space, observations $o \\in \\mathcal{O}$ (discrete or continuous) $\\mathcal{T}$ - transition operator, a tensor! $\\mathcal{E}$ - emission probability $p(o_{t}|s_{t})$ $r$ - reward function, $r: \\mathcal{S}\\times \\mathcal{A} =\\lbrace (s,a)| s \\in \\mathcal{S}, a \\in \\mathcal{A}\\rbrace \\rightarrow \\mathbb{R}$ Goal of reinforcement learning We will talk about partially observed later, for now let\u0026rsquo;s just say our policy is conditioned on $s$, and $\\theta$ corresponds to the parameters of the policy. If policy is a kind of neural network, we could find that $\\theta$ denotes the parameters of this deep neural net. The state input policy and action go into the transition probability $p(\\mathbf{s}^{\\prime}|\\mathbf{s}, \\mathbf{a})$, which produces the next state.\nIn this process, we could write down a probability distribution over trajectories, which are sequences of states and actions by using chain rules.\n$$ \\begin{equation} \\underbrace{ p_{\\theta}(\\mathbf{s}_{1},\\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T} ) }_{ p_{\\theta}(\\tau) } = p(\\mathbf{s}_{1}) \\prod^{T}_{t=1}\\underbrace{ \\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})p(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t}) }_{ \\text{Markov chain on} (\\mathbf{s},\\mathbf{a}) } \\end{equation} $$ For now we assume that our control problem has a finite horizon, implying that the decision-making task is limited to a fixed time step $T$ and then ends. We could factorize it by using the chain rule in terms of probability distribution that we've already defined. For notational brevity, sometimes $p_{\\theta}(\\mathbf{s}_{1},\\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T} )$ will be rewritten by $p_{\\theta}(\\tau)$. Having define the distribution, we could actually define an objective as an expected value under the trajectory distribution for reinforcement learning tasks. (P.S. this equation could be used in finite horizon situation) $$ \\begin{equation} \\theta^{*} = \\arg \\max_{\\theta} E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\sum_{t}r(\\mathbf{s}_{t},\\mathbf{a}_{t}) \\right] \\end{equation} $$ We would like to find the paraments $\\theta$, which is our goal. By doing this, we aim to maxmize the expected value of the sum of rewards over the trajectory. Generally, $\\pi_{\\theta}(\\mathbf{a} _{t}|\\mathbf{\\mathbf{s} _{t}})$ given $\\mathbf{s} _{t}$ allows us to get a distribution of our actions condition on states. What we can do is to group state and action together into a kind of augmented state. And now the augments states actually form a Markov chain as the following figure. We could write down the enhanced transition operator (personal nick name for this operator) in this augmented Markov chain.\n$$ p((\\mathbf{s}_{t+1},\\mathbf{a}_{t+1})|(\\mathbf{s}_{t},\\mathbf{a}_{t})) = p(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})\\pi_{\\theta}(\\mathbf{a}_{t+1},\\mathbf{s}_{t+1}) $$ Next we could do is to write the objective by linearity of expectation as the sum over time of the expected values under the state action marginal $p_{\\theta}(\\mathbf{s} _{t}, \\mathbf{a} _{t})$ in this Markov chain, cause this transition operator is product of transition and policy with good linear mathematical structure.\n$$ \\begin{equation} \\theta^{*} = \\arg \\max_{\\theta}\\sum^{T}_{t = 1} E_{(\\mathbf{s}_{t}, \\mathbf{a}_{t})\\sim p_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})}[r(\\mathbf{s}_{t}, \\mathbf{a}_{t})] \\end{equation} $$ This formulation simplifies the computation by focusing solely on the expectation of rewards at each time step, rather than considering the expectation over the entire trajectory. Infinite horizon case What if $T \\rightarrow \\infty$ ?\nFirst, if that happens our objective might become not so clear. Please imagine that you have a sum of infinite positive numbers rewards, which is infinity. We need to make the objective finite, using discount will solve this problem，or choose a mathematical operation divide by $T$ so that the sum of the expectations of the marginals becomes dominated by the stationary distribution terms.\nNow, we could ask a more specific question, does $p(\\mathbf{s} _{t},\\mathbf{a} _{t})$ converge to a stationary distribution?\nIf this is possible, that means we could write the stationary distribution and it must obey this equation, $\\mu=\\mathcal{T}\\mu$. We could prove this conclusion under a few technical assumptions namely ergoicity and chain being aperiodic. Ergoicity means every state can be reached from every other state with non-zero probability, it prevents a situation where if you start in one part of the MDP, you might never reach another one. It dives us into characteristic equation in linear algebra to solve\n$$(\\mathcal{T}-\\mathbf{I})\\mu=0$$ We could find $\\mu$ by finding the eigenvector with eigenvalue one for the matrix defined by $t$. And $\\mu$ is eigenvector of $\\mathcal{T}$ with eigenvalue $1$, it always exists under ergodic and aperiodic assumption. When we consider the average reward case divided by $T$, the limit as $t$ approaches infinity will be the expected value of the reward. Formally, this is expressed as\n$$ \\begin{equation} \\begin{aligned} \\theta^{*} \u0026= \\arg \\max_{\\theta} \\frac{1}{T} \\sum_{t=1}^{T} E_{(\\mathbf{s}_{t}, \\mathbf{a}_{t}) \\sim p_{\\theta}(\\mathbf{s}, \\mathbf{a})} [r(\\mathbf{s}_{t}, \\mathbf{a}_{t})] \\rightarrow \\color{red}{(E_{(\\mathbf{s}, \\mathbf{a}) \\sim p_{\\theta}} [r(\\mathbf{s}, \\mathbf{a})])} \\end{aligned} \\end{equation} $$ In simpler way, as $T$ (the number of time steps) becomes very large, the average reward converges to the expected value of the reward given the policy $p_{\\theta}$.\nAlgorithm The reinforcement learning algorithms presented in this discussion, all of them would have more or less the same anatomy. The first part of our workflow is to generate samples, reinforcement learning is about learning through trial and error. Simply speaking, samples are the paths we have taken through the environment to reach our current position. Imagine that we have collected some samples. The next step is to find a suitable model for the dynamics in our model-based reinforcement learning algorithm. Then we turn into the final part, which is where you actually change your policy to make it better.\n$$ \\mathcal{J}(\\theta) = E_{\\pi}\\left[ \\sum_{t}r_{t} \\right] \\approx \\frac{1}{N}\\sum^{N}_{i = 1}\\sum_{t}r_{t}^{i} $$ $$ \\theta \\leftarrow \\theta +\\alpha \\nabla_{\\theta}\\mathcal{J}(\\theta) $$\ncomming soon\u0026hellip;\nValue functions comming soon\u0026hellip;\n","permalink":"https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/","summary":"Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.","title":"Introduction to reinforcement learning"},{"content":"背景介绍 孤子的发现应追溯到1834年的夏日，英国科学家 J.S.Russel 骑马正沿着一条运河岸道旅行，偶然发现在狭窄的河床中行走的船突然停止前进，被船体带动的水团积聚在船头周围并剧烈地翻动着. 不久，一个圆形且轮廓分明的巨大孤立波峰开始形成，并急速离开船头向前运动. 波长约 10 米，高约 0.5 米，在行进中波的形状和速度并无明显变化，以后高度逐渐下降，在跟踪至三公里后，终于消失在蜿蜒的河道上. 这次发现的奇特景观促使 Russel 开始广泛的水波实验研究. 他称这沿着狭窄通道传播的、保持形状和速度不变的水包为“伟大的孤立波”，并意识到这是流体力学中某方程的稳定解. 然而, 由于当时数学水平与计算技术的限制，罗素的研究没有取得满意的进展，直到1895年，荷兰著名数学家 Korteweg 和他的学生 de Vries 在对孤波进行全面分析后指出这种波可近似为小振幅的长波，并以此建立了浅水波运动方程1：\n$$ \\begin{equation} \\frac{\\partial \\eta}{\\partial \\tau}=\\sqrt{\\frac{g}{h}} \\frac{\\partial}{\\partial \\xi}\\left(\\frac{3}{4} \\eta^2+\\alpha \\eta+\\frac{\\sigma}{2} \\frac{\\partial^2 \\eta}{\\partial \\xi^2}\\right), \\label{eq_1} \\end{equation} $$\n其中, $\\sigma=\\frac{1}{3} h^3-\\frac{T h}{\\rho g}, \\eta$ 为波面高度, $h$ 为水深, $g$ 为重力加速度, $\\rho$ 是水的密度, $\\alpha$ 是与水的匀速流动有关的小常数, $T$ 是水的表面张力. 此后 Korteweg 和 de Vries 利用行波法求出与 Russel 描述一致的孤波解, 争论才告终止.\n如果作以下变换\n$$ \\begin{equation} t = \\frac{1}{2} \\sqrt{\\frac{g}{h \\sigma}}\\tau, \\quad x = -\\frac{\\xi}{\\sqrt{\\sigma}}, \\quad u=\\frac{1}{2}\\eta +\\frac{1}{3}\\alpha \\end{equation} $$\n则 Eq. $\\eqref{eq_1}$ 可写成标准的形式\n$$ \\begin{equation} u_t+u_{x x x}+6 u u_x=0 \\label{eq_3} \\end{equation} $$\n后人为了纪念这两位伟大的学者对孤波作出的贡献将 Eq. $\\eqref{eq_1}$ 或 Eq. $\\eqref{eq_3}$ 称为 KdV 方程.\nKdV 方程的推导 1968年，Lax 曾提出了一种用线性算子导出孤子方程的方法 2, 具体计算方法如下. 首先线性算子 $\\mathcal{L}$ 满足\n$$ \\begin{equation} \\mathcal{L} \\phi=\\lambda \\phi . \\label{eq_4} \\end{equation} $$\n其中 $\\lambda$ 为谱参数, 如果只考虑等谱情况 ( $\\lambda$ 与时间无关), 即 $\\lambda_t=0$，其次 $\\phi$ 还满足\n$$ \\begin{equation} \\phi_t=\\mathcal{A} \\phi, \\label{eq_5} \\end{equation} $$\n其中 $\\mathcal{A}$ 也是线性算子. 将 Eq. $\\eqref{eq_4}$ 对 $t$ 求导, 同时结合 Eq. $\\eqref{eq_5}$ 有:\n$$ \\begin{equation} \\mathcal{L}_t \\phi+\\mathcal{L}\\mathcal{A} \\phi=\\mathcal{A}\\mathcal{L} \\phi \\end{equation} $$\n从而有\n$$ \\begin{equation} \\mathcal{L}_t=\\mathcal{A}\\mathcal{L} - \\mathcal{L}\\mathcal{A} = [\\mathcal{A}, \\mathcal{L}] . \\end{equation} $$\n这便是著名的 Lax 方程, 其中 $\\mathcal{L}, \\mathcal{A}$ 称为 Lax 对. 这里给出一种由 Lax 对进行推导得出 KdV 方程的较为简单的方式， 取 $\\mathcal{L}$ 为 Hamilton 算子 $\\mathcal{L}=\\partial^2 _{x} - u(x, t)$ (实际上Lax对确实和薛定谔方程是有联系的)， $\\mathcal{A}$ 为反对称算子 $\\mathcal{A}=\\alpha \\partial_x^3 + B(x,t) \\partial_x+ C(x,t) $ ， 其中 $\\alpha$ 是常数， $B(x, t), C(x, t)$ 是待定的项, 则\n$$ \\begin{equation} \\begin{aligned} \\mathcal{A}\\mathcal{L} \u0026 =\\left(\\alpha \\partial_x^3 + B(x,t) \\partial_x+ C(x,t) \\right)\\left(\\partial^2 _x - u\\right) \\\\ \u0026 = \\alpha \\partial_x^5 - \\alpha(u_{xxx}+3u_{xx}\\partial_{x} + 3u_{x}\\partial_{x}^2 + u\\partial_{x}^3) \\\\ \u0026 + B(\\partial_{x}^3-u_{x}-u\\partial_{x}) + C(\\partial_{x}^2-u)\\\\ \\mathcal{L}\\mathcal{A} \u0026 =\\left(\\partial^2 _x - u\\right)\\left(\\alpha \\partial_x^3 + B(x,t) \\partial_x+ C(x,t) \\right) \\\\ \u0026 = \\alpha \\partial_x^5 +(B_{xx}\\partial_{x} + 2B_{x}\\partial_{x}^2 + B \\partial_{x}^3) + (C_{xx}+2C_{x}\\partial_{x} + C\\partial_{x}^2) \\\\ \u0026 - u\\left(\\alpha \\partial_x^3 + B \\partial_x+ C \\right) \\end{aligned} \\end{equation} $$ 由这两个算子便有\n$$ \\begin{equation} \\begin{aligned} u _{t} \u0026 =\\mathcal{A}\\mathcal{L} -\\mathcal{L}\\mathcal{A} \\\\ \u0026 = -(3 \\alpha u_{x}+2B_{x})\\partial_{x}^2 -(3\\alpha u_{xx} + B_{xx}+2C_{x})\\partial_{x} +\\alpha u_{xxx} - Bu_{x} -C_{xx}. \\end{aligned} \\end{equation} $$ 其中等式右边有关 $\\partial_{x}$ 的项都为 $0$，可得\n$$ \\begin{equation} \\begin{aligned} \u0026amp;3 \\alpha u_{x}+2B_{x} = 0 \\ \u0026amp;3\\alpha u_{xx}+B_{xx}+2C_{x} = 0 \\end{aligned} \\end{equation} $$\n进行积分后可以得到\n$$ \\begin{equation} \\begin{aligned} \u0026amp;B = -\\frac{3}{2} \\alpha u \\ \u0026amp;C = -\\frac{3}{2} \\alpha u_{x}+\\frac{1}{2} B_{x} = -\\frac{3}{4}\\alpha u_{x} \\end{aligned} \\end{equation} $$\n可以得到 $u_{t} = \\alpha u_{xxx} - Bu_{x} -C_{xx}= \\frac{1}{4}\\alpha u_{xxx}+\\frac{3}{2}\\alpha u u_{x}$， 取 $\\alpha=-4$, 即为 $u_t+u_{x x x}+6 u u_x=0$.\nKdV方程求解 双曲正切法 首先, 运用行波法将所求解的非线性偏微分方程转化为非线性常微分方程, 即令 $\\xi=c(x-v t)$, 则 $u(x, t)=U(\\xi)$ ，有\n$$ \\begin{equation} \\frac{\\partial}{\\partial t} \\rightarrow-c v \\frac{\\mathrm{d}}{\\mathrm{d} \\xi}, \\frac{\\partial}{\\partial x} \\rightarrow c \\frac{\\mathrm{d}}{\\mathrm{d} \\xi} \\end{equation} $$\n然后, 将常微分方程积分, 直到微分方程中至少有一项不含导数项且尽可能 使方程中导数项具有较低阶数为止，积分常数都取为零. 引入 $Y=\\tanh (\\xi)$ 作为新的独立变量, 相应的导数变为\n$$ \\begin{equation} \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d} \\xi} \\rightarrow \u0026\\left(1-Y^2\\right) \\frac{\\mathrm{d}}{\\mathrm{d} Y} \\\\ \\frac{\\mathrm{d}^2}{\\mathrm{d} \\xi^2} \\rightarrow \u0026\\left(1-Y^2\\right)\\left[-2 Y \\frac{\\mathrm{d}}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2}{\\mathrm{d} Y^2}\\right] \\\\ \\frac{\\mathrm{d}^3}{\\mathrm{d} \\xi^3} \\rightarrow \u0026-2 Y\\left(1-Y^2\\right)\\left[-2 Y \\frac{\\mathrm{d}}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2}{\\mathrm{d} Y^2}\\right] \\\\ \u0026+\\left(1-Y^2\\right)\\left[-2\\frac{\\mathrm{d}}{\\mathrm{d} Y}-2 Y \\frac{\\mathrm{d}^2}{\\mathrm{d} Y^2}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^3}{\\mathrm{d} Y^3}\\right] \\end{aligned} \\end{equation} $$ 求解过程 对于广义 KdV 方程, 运用双曲正切法进行求解3. 现在, 我们通过此方法对 Eq. $\\eqref{eq_3}$ 中的形式进行求解. 通过 $\\frac{\\partial}{\\partial t} \\rightarrow-c v \\frac{\\mathrm{d}}{\\mathrm{d} \\xi}, \\frac{\\partial}{\\partial x} \\rightarrow c \\frac{\\mathrm{d}}{\\mathrm{d} \\xi}$ 变换为\n$$ \\begin{equation} -v U_{\\xi}+6 U U_{\\xi}+c^2 U_{\\xi \\xi \\xi}=0, \\end{equation} $$\n两边进行积分, 可以得到\n$$ \\begin{equation} -v U+3 U^2+c^2 U_{\\xi \\xi}=0 \\end{equation} $$\n引入 $Y=\\tanh (\\xi)$ 作为新的独立变量,方程相应的可以变化为\n$$ \\begin{equation} -v S(Y)+3 S^2(Y)+c^2\\left(1-Y^2\\right) \\left(-2 Y \\frac{\\mathrm{d} S(Y)}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2 S(Y)}{\\mathrm{d} Y^2}\\right)=0 \\label{eq_12} \\end{equation} $$\n对于 $S=\\sum_{m=0}^m a_m Y^m$, 参数 $M$ 是通过平衡第二项(非线性)的阶数来确定. 不妨设 $S(Y)=\\gamma-\\gamma Y^2$, 代入 Eq. $\\eqref{eq_12}$进行化简可以得到\n$$ \\begin{equation} -v+3\\left(\\gamma-\\gamma Y^2\\right)+c^2 \\left(-2 Y \\frac{\\mathrm{d}\\left(1-Y^2\\right)}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2\\left(1-Y^2\\right)}{\\mathrm{d} Y^2}\\right)=0 \\end{equation} $$\n比较 $Y^0$ 和 $Y^2$ 的系数:\n$$ \\begin{equation} \\begin{gathered} -v+3 \\gamma-2 c^2=0 \\ -3 \\gamma+4 c^2+2 c^2=0 \\end{gathered} \\end{equation} $$\n现有三个末知数 $(v, \\gamma, c)$ 和两个方程, 所以可以选择 $c$ 作为自由参数. 发现其它变量为 $\\gamma=2 c^2, v=4 c^2$.\n最后, KdV 方程的孤波解为 3 :\n$$ \\begin{equation}\\begin{gathered} u(x,t) =2c^2\\left\\{1-\\tanh^2\\left[c\\left(x-4c^2t\\right)\\right]\\right\\} \\\\ =2c^2\\sec h^2\\left[c\\left(x-4c^2t\\right)\\right] \\end{gathered}\\end{equation} $$ P.S. 其中用 Lax pair 推导的方式几经修改，修正了一些细节内容.\n参考资料 Korteweg, Diederik Johannes, and Gustav De Vries. \u0026ldquo;On the change of form of long waves advancing in a rectangular canal, and on a new type of long stationary waves.\u0026rdquo; The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 39.240 (1895): 422-443.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLax, Peter D. \u0026ldquo;Integrals of nonlinear equations of evolution and solitary waves.\u0026rdquo; Communications on pure and applied mathematics 21.5 (1968): 467-490.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMalfliet, Willy. \u0026ldquo;Solitary wave solutions of nonlinear wave equations.\u0026rdquo; American journal of physics 60.7 (1992): 650-654.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://kairoswang.github.io/posts/2023-07-09-kdv_equation/","summary":"背景介绍 孤子的发现应追溯到1834年的夏日，英国科学家 J.S.Russel 骑马正沿着一条运河岸道旅行，偶然发现在狭窄的河床中行走的船突然停止前进，被船体带动的","title":"KdV 方程求解及其背景"},{"content":"从霜花到海岸线，自然界中分形无处不在，近现代物理学中同样不断能够找到分形的影子. 本文回顾了通过类比 Weierstrass 函数，可以从量子气体中寻求到相应的动力学分形. 随后，主要详细讲解了一种有趣的量子分形现象 —Hofstadter蝴蝶. 对于二维晶格的体系，加上对应的磁通，提取系统本征能量后便可以得到这种美丽的分形图案，最后举例了一些最新的相关的研究情况. (P.S. 这篇文章是我的课程设计，再次回看的时候添加了一些新内容，对以前的东西也有了新的理解.)\n引言 借用一句古语——“一花一世界，一树一菩提”，说的是从细微之处洞察宏观的哲学思考，而“一即是全，全即是一”，这是对分形最传神的表达. 每当看到那一张张分形图案时，不免思考这样美丽的图案是如何生成的，“分形”一出现便引起了年幼时我的兴趣. 现如今，随着电脑技术的兴起，分形被广泛运用到复杂图像的产生和处理上，其中包括大量电影里的星球表面，山川起伏的画面.\n1861年，德国数学家魏尔施特拉斯 (Karl Theodor Wilhelm Weierstrass, 1815-1897) 发现了一个函数：\n$$ \\begin{equation} f(x)=\\sum_{n=0}^{\\infty} a^{n} \\cos \\left(b^{n} \\pi x\\right) \\end{equation} $$\n其中 $0\u0026lt;a\u0026lt;1$，$b$为正奇数，且需要满足 $ab\u0026gt;1+\\frac{3}{2}\\pi$，这个函数以及它处处连续而又处处不可导的证明首次出现在魏尔施特拉斯于1872年7月18日在普鲁士科学院出版的一篇论文中.\n从现在的数学角度看，毫无疑问这是一条分形曲线，然而“分形”这个概念直到一个世纪之后的1975年才由 Mandelbrot 提出来1. 在这里尝试一个简单的画图，Weierstrass函数有无穷项，但是随着 $n \\to \\infty, a^{n} \\cos \\left(b^{n} \\pi x\\right)\\to 0$，故可用有限项作为代替进行处理.\nFig. 1: Weierstrass 函数可视化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 x=linspace(-2,2,1000); S1=outputweier(0.5,1); S2=outputweier(0.5,2); S3=outputweier(0.5,3); subplot(3,1,1) plot(x,S1); subplot(3,1,2) plot(x,S2); subplot(3,1,3) plot(x,S3); xlabel(\u0026#39;\\it{x}\u0026#39;); %% function [S]=outputweier(a,b)%求和函数 syms f n x=linspace(-2,2,1000); f=a^n*cos(b^n*pi*x); S=symsum(f,n,0,100); end\t量子气体中的动力学分形 能量标度 上述的函数我们看起来如此奇怪，但却可以把它用在量子气体中. 类比于传统意义上的离散标度对称性 $W(b x) \\simeq a^{-1} W(x)$ (简而言之就是缩放后的自相似，discrete scaling symmetry)，在谐振二体相互作用的三粒子系统中，也有类似的 2：\n$$ \\begin{equation} E_{n+1} \\simeq \\lambda^{2} E_{n} \\end{equation} $$\n利用能量标度关系，构造 Loschmidt 振幅 $\\mathcal{L}(t)$. 受“分形”函数的启示，与量子系统的相似性启发可以构造出相应的类 Weierstrass 函数：\n$$ \\begin{equation} \\mathcal{L}(t) \\propto \\sum_{n=0}^{N} \\lambda^{-n D} e^{-(i / \\hbar) \\lambda^{2 n} E_{0} t} \\end{equation} $$\nLoschmidt 振幅“分形”图像 通过类比的方法，选择初始波函数、测量、维度、适当的能量和时间尺度的要求，便可以用超冷量子气体来实现在时域上表现出分形行为2.\nFig. 2: Loschmidt echo 振幅 $\\mathcal{L}(t)$实数部分，按比例放大后呈现出自相似特性. (Image source: Gao et al. 2019)\n数值模拟表明，所有这些要求可以很容易地同时满足实际参数在冷原子气体. 目前的计算是基于单个粒子图，忽略了粒子间的相互作用.\n霍夫斯塔特蝴蝶(Hofstadter\u0026rsquo;s Butterfly) 1976年，Douglas Hofstadter 在研究 Bloch 电子在外磁场作用下的量子现象时发现电子的能量和磁场强度可形成分形图像.\n朗道能级 事实上，从现在的眼光来看，Hofstadter\u0026rsquo;s Butterfly本身就是朗道能级的二维格点版本. 带电粒子能量在一系列分立的数值中取值，形成朗道能级. 求解外加磁场下的薛定谔方程，从而得到电子能级的分布，即为朗道能级. 外加磁场为均匀磁场，沿 $z$ 方向，即 $\\textbf{B}=(0,0,B)$.\n由 $\\mathbf{B}=\\nabla \\times \\hat{\\mathbf{A}}$，取朗道规范可得磁矢势 $\\hat{\\mathbf{A}}=(-B y , 0, 0)$，式中 $\\lvert {B} \\rvert$，这样的规范下电子的正则动量为 $\\hat{\\textbf{p}}+q \\hat{\\textbf{A}}$，电子的哈密顿量可以写为：\n$$ \\begin{equation} \\hat{H}=\\frac{(\\hat{p}+q \\hat{A})^{2}}{2 m}=\\frac{1}{2 m}\\left[\\left(\\hat{p_{x}}-q B y\\right)^{2}+\\hat{p_{y}}^{2}+\\hat{p_{z}^{2}}\\right] \\end{equation} $$\n$\\hat{H}$ 不显含 $x,z$，则 $\\hat{H}$ 与 $\\hat{p_x},\\hat{p_z}$ 对易，可选取 $(\\hat{H},\\hat{p_x},\\hat{p_z})$ 力学量完全集，共同本征函数形式可得为 $\\psi(x, y, z)=e^{\\frac{i}{\\hbar}\\left(p_{x} x+p_{z} z\\right)} \\phi(y)$，代入 $\\hat{H} \\psi=E \\psi$，并做变量替换 $\\xi=y+\\frac{p_{x}}{q B},\\omega=\\frac{q B}{m},E^{\\prime}=E-\\frac{p_{z}^{2}}{2 m}$，得到如下方程：\n$$ \\begin{equation} \\left[-\\frac{\\hbar^{2}}{2 m} \\frac{\\mathrm{d}^{2}}{\\mathrm{d} \\xi^{2}}+\\frac{1}{2} m \\omega^{2} \\xi^{2}\\right] \\phi(\\xi)=E^{\\prime} \\phi(\\xi) \\end{equation} $$\n该方程在形式上与一维谐振子势的定态方程完全一致，求解步骤相同，可以得到能级的分立解为：\n$$ \\begin{equation} E_{n}^{\\prime}=\\left(n+\\frac{1}{2} \\hbar \\omega\\right), n \\in N \\end{equation} $$\n量子霍尔效应与Hofstadter蝴蝶的联系 所谓量子霍尔效应与经典理论有何不同呢？在经典理论下，自由电子在电场 $\\mathbf{E}$ 和磁场 $\\mathbf{B}$ 的作用下的运动方程可写为：\n$$ \\begin{equation} m\\left(\\frac{\\mathrm{d}}{\\mathrm{d} t}+\\frac{1}{\\tau}\\right) \\vec{v}=-e(\\vec{E}+\\vec{v} \\times \\vec{B}) \\end{equation} $$\n其中 $\\tau$ 为电子的弛豫时间，$\\frac{m \\vec{v}}{\\tau}$ 这一项表示电子运动过程中的碰撞效应，在一个二维系统中写为分量形式，当到达稳态时速度不随时间变化，已知电流密度 $J=n e v$，并令$\\sigma=\\lvert e \\rvert n v, v=\\lvert e \\rvert \\tau/ m$，写成矩阵形式为：\n$$ \\begin{equation} \\left[\\begin{array}{l} J_{x} \\\\ J_{y} \\end{array}\\right]=\\sigma\\left[\\begin{array}{cc} 1 \u0026 -\\mu B \\\\ \\mu B \u0026 1 \\end{array}\\right]^{-1}\\left[\\begin{array}{c} E_{x} \\\\ E_{y} \\end{array}\\right] \\end{equation} $$ 当施加磁场后会使得矩阵出现了非对角元. 写成电阻形式 $E=\\rho_{2 \\times 2} J$，可以得到经典霍尔效应的电阻率：\n$$ \\begin{equation} \\rho_{x x}=\\sigma^{-1}, \\rho_{y x}=-\\rho_{x y}=\\frac{B} {|e|} n \\end{equation} $$\n然而，1980年德国物理学家 Klaus von Klitzing 发现在极低温、强磁场条件下，二维电子气的霍尔电阻出现了平台化特征，其值等于量子化电阻值 $(h/e^2)$ 的整数倍. 在霍尔电阻表现为量子化平台的同时，纵向电阻则趋于零3. 在实际实验研究中，更常采用的是固定二维电子气的密度，在维持流过样品的电流恒定的条件下，测量霍尔电压；随外加磁场的变化，可见在低磁场下霍尔电阻的确与外磁场的磁感应强度成线性关系；但在高磁场区则得到霍尔电阻的平台状结构.\nFig. 3: 横向霍尔电阻的平台结构与纵向电阻的SdH振荡. (Image source: Klitzing et al. 1980)\n1976年，Douglas Hofstadter在研究Bloch电子在外磁场作用下的量子现象时发现电子的能量和磁场强度可形成分形图像. 在他的博士论文中研究了晶格里跃迁几率幅为复数的电子行为，这种复跃迁破坏了时间空间反演对称，后续又引出了Chern绝缘体的话题. 如图所示，蝴蝶能谱的整数区域其实就是霍尔电导率的量子化台阶 4. 正是这种有趣的量子分形现象———Hofstadter蝴蝶.\nFig. 4: 横轴为复跃迁的复角度，纵轴为费米能. 用整数量子标记间隙，用相同颜色的间隙表示具有相同霍尔电导率量子数的量子霍尔态. (Image source: Satija et al. 2018)\n晶格中的“分形”状能谱 二维方格子 首先，考虑如一个二维晶体体系，每个格点(交叉点)上放一个各向同性的电子轨道能级. 体系近似的哈密顿量可写成：\n$$ \\begin{equation} H=\\sum_{m, n} H_{m, n}^{0}+\\sum_{m, n, m^{\\prime}, n^{\\prime}} \\frac{e^{2}}{4 \\pi \\varepsilon\\left|\\hat{\\vec{r}}_{m, n}-\\hat{\\vec{r}}_{m^{\\prime}, n^{\\prime}}\\right|} \\end{equation} $$ 每个格点上都有一个初始波函数 $H_{m, n}^{0} \\vert m, n\\rangle=\\mu \\vert m, n\\rangle$，且认为不同格点之间的波函数相互正交 $\\left\\langle m^{\\prime}, n^{\\prime} \\mid m, n\\right\\rangle=\\delta_{m^{\\prime}, m} \\delta_{n^{\\prime}, n}$. 直接可以计算哈密顿量矩阵，然后对角化哈密顿量，便得到了体系的本征能量以及本征波矢. 矩阵元为5：\n$$ \\begin{equation} \\left\\langle m, n|H| m^{\\prime}, n^{\\prime}\\right\\rangle=\\mu \\delta_{m, m^{\\prime}} \\delta_{n, n^{\\prime}}+\\left\\langle m, n\\left|\\frac{e^{2}}{4 \\pi \\varepsilon\\left|\\hat{\\vec{r}}_{m, n}-\\hat{\\vec{r}}_{m^{\\prime}, n^{\\prime}}\\right|}\\right| m^{\\prime}, n^{\\prime}\\right\\rangle \\end{equation} $$ 换用投影算符的形式来描述这样一个哈密顿量矩阵如下：\n$$ \\begin{equation} \\hat{H}=\\sum_{m} \\mu|m, n\\rangle\\langle m, n|+\\sum_{m, n ; m^{\\prime}, n^{\\prime}}{}^{\\prime}\\left(t_{m n, m^{\\prime} n^{\\prime}}|m, n\\rangle\\left\\langle m^{\\prime}, n^{\\prime}\\right|+h . c\\right) \\label{eq_12} \\end{equation} $$\n其中，$t _{m n, m^{\\prime} n^{\\prime}} = \\left\\langle m, n \\lvert \\frac{e^{2}}{4 \\pi \\varepsilon \\lvert \\hat{\\vec{r}} _{m, n}-\\hat{\\vec{r}} _{m^{\\prime}, n^{\\prime}} \\rvert } \\rvert m^{\\prime}, n^{\\prime}\\right\\rangle$, Eq. $\\eqref{eq_12}$ 中 $\\sum{}^{\\prime}$ 表示 $m \\neq m^{\\prime}; n \\neq n^{\\prime}$. 考虑到不同格点之间的波函数交叠很小，一般只考虑最近邻格点之间的交叠，认为处理最近邻的相互作用，由此哈密顿量可写成：\n$$ \\begin{equation} \\hat{H}=\\sum_{m} \\mu|m, n\\rangle\\langle m, n|+\\sum_{m, n}t_{x}|m+1, n\\rangle\\left\\langle m, n\\left|+t_{y}\\right| m, n+1\\right\\rangle\\langle m, n|+h . c \\end{equation} $$\n在数值算的时候，假设格点上没有初始波函数，相应的哈密顿量取其一部分为，考虑临近相互作用的形式用直积(方便计算)可以写为\n$$ \\begin{equation} H=\\sum_{m, n} t_{x}|m\\rangle\\langle m+1|\\otimes| n\\rangle\\left\\langle n\\left|+t_{y}\\right| m\\right\\rangle\\langle m|\\otimes| n\\rangle\\langle n+1|+h.c \\end{equation} $$\n加静磁场后的方格子 当静磁场对该晶格系统产生影响时，情况又会发生不一样的变化. 电磁场中自由电子的哈密顿量代入薛定谔方程后可以写为6：\n$$ \\begin{equation} \\left[\\frac{(\\hat{\\vec{p}}+e \\vec{A})^{2}}{2 m}-e \\varphi\\right] \\psi=i \\hbar \\frac{\\partial}{\\partial t} \\psi \\end{equation} $$\n如果我们把波函数写成$\\psi^{\\prime}(\\vec{r})=e^{-i \\frac{e}{\\hbar} \\int_{r_{0}}^{\\vec{r}} \\vec{A}\\left(\\vec{r}^{\\prime}\\right) d \\vec{r}^{\\prime}} \\psi(\\vec{r})$，如果对它做一个空间平移，由于磁场的存在，这样的一个空间平移会给波函数带来一个额外的相位. 然后让每个格子都有一个 $\\phi$ 的磁通时，把哈密顿量写成矩阵的形式后就可以直接算本征值，提取系统的本征能量，便可以绘制出Hofstadter蝴蝶.\nFig. 5: 晶格系统加上磁通. (Image source: Schematic of the experiments by Aidelsburger et al.)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 clear all; close all; % Set up simulation parameters hop_energy = 1; num_sites = 10; num_phases = 200; site_indices = 1:num_sites; phase_values = linspace(0, 2, num_phases); % Initialize energy array energy_spectrum = zeros(num_phases, num_sites^2); % Compute energy spectrum for phase_index = 1:num_phases phase = phase_values(phase_index) * pi; hop_matrix = diag(ones(1, num_sites - 1), 1) + diag(ones(1, num_sites - 1), -1); phase_matrix = diag(exp(1j * phase * site_indices)); hamiltonian = kron(eye(num_sites), hop_matrix * hop_energy) + ... kron(diag(ones(1, num_sites - 1), 1), phase_matrix) + ... kron(diag(ones(1, num_sites - 1), -1), phase_matrix\u0026#39;); [~, eigvals] = eig(hamiltonian); energy_spectrum(phase_index, :) = diag(eigvals); end % Plot energy spectrum figure(); hold on; for band_index = 1:num_sites^2 band_energies = energy_spectrum(:, band_index); band_energies(end) = NaN; colors = band_energies; patch(phase_values / 2, band_energies, colors, \u0026#39;edgecolor\u0026#39;, \u0026#39;flat\u0026#39;, \u0026#39;facecolor\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;Linewidth\u0026#39;, 1); end hold off; box on; colormap(hsv); axis square; xlim([0, 1]); ylim([-4, 4]); Fig. 6: 正方形晶格有限截面的能谱数值模拟图：(a)$2×2$网格; (b)$3×3$网格; (c)$4×4$网格; (d)$6×6$网格; (e)$7×7$网格; (f)$10× 10$网格.\n特别值得一提的是,如果对实空间上离散的Schródinger方程进行傅里叶变换，得到如下方程：\n$$ \\begin{equation} 2 \\cos \\left(k_{1} a+\\frac{2 \\pi p r}{q}\\right) \\tilde{\\psi}_{r}(\\mathbf{k})+e^{i k_{2} a} \\tilde{\\psi}_{r+1}(\\mathbf{k})+e^{-i k_{2} a} \\tilde{\\psi}_{r-1}(\\mathbf{k})=-\\frac{E(\\mathbf{k})}{t} \\tilde{\\psi}_{r}(\\mathbf{k}) \\label{eq_16} \\end{equation} $$ 其中 $\\mathbf{k}=\\left(k_{1}, k_{2}\\right)$，Eq. $\\eqref{eq_16}$就是Harper方程. 用数值方法求解时，如果我们不断改变 $\\Phi / \\Phi_{0}$ (即朗道能级简并度)时，结果也可以得到这样一个美丽的分形结构.\n朗道扇形能谱 对于无限二维方形晶格的紧束缚模型，具有能量色散关系：\n$$ \\begin{equation} E(\\mathbf{k})=-2|t|\\left(\\cos \\left(k_{x} \\alpha \\right)+\\cos \\left(k_{y} \\alpha \\right)\\right) \\label{eq_17} \\end{equation} $$\n由于能量范围是 $-4 \\lvert t \\rvert \u0026lt; E \u0026lt; 4 \\lvert t \\rvert$，带宽为 $8\\lvert t \\rvert$，在二维紧束缚带的底部附近由Eq. $\\eqref{eq_17}$ 描述，能量近似为类自由电子，约化质量为7：\n$$ \\begin{equation} E(\\mathbf{k}) \\approx-4|t|+\\frac{\\hbar^{2}}{2 m^{*}}\\left(k_{x}^{2}+k_{y}^{2}\\right) \\end{equation} $$\n其中，有效质量为 $m^{\\ast}=\\hbar^{2} /\\left(2 \\lvert t \\rvert \\alpha^{2}\\right)$，回旋频率可以写为 $\\omega_{c}=\\frac{e B}{m^{\\ast}}=\\frac{4 \\pi \\alpha \\lvert t \\rvert}{\\hbar}$，因此，靠近谱带底部的朗道能级可以写成\n$$ \\begin{equation} \\frac{E}{|t|}=-4+4 \\pi \\alpha\\left(n+\\frac{1}{2}\\right) \\label{eq_19} \\end{equation} $$\nFig. 7: (a)$10× 10$方阵区的能谱; (b)朗道扇形能谱. 对于半满带的情况，化学势$\\mu$设为零.\n根据图(b)所示,即为Eq. $\\eqref{eq_19}$给出的朗道水平扇面. 正是这种朗道能级的扇形引起了德哈斯-范阿尔芬效应(De Haas-van Alphen effect,纯金属晶体的磁化强度随外加磁场的增加而发生振荡的现象)，在这些效应占据的朗道能级中，化学势 $\\mu$ 和费米能量 $E_p$ 下降到下一个朗道能级. 同样，有限系统成键轨道的能量在稠密区域增加，在稀疏区域迅速下降，这些轨道在二维系统中表现出磁振荡的特征.\n实验相关 在凝聚态物理学中，Hofstadter蝴蝶理论描述了晶格中磁场中不相互作用的二维电子的光谱特性，并在整数量子霍尔效应理论和拓扑量子数理论中起着重要的作用，但由于该现象需要苛刻的实验条件，直到20年后才在实验上观察到这样一幅美丽的景象.\n1997年，Hofstadter蝴蝶在由一组散射体装备的微波波导的实验中得以再现8. 正是由于散射体的微波波导的数学描述与磁场中的Bloch波之间的相似性，使得散射体周期序列的Hofstadter蝴蝶得以再现.\nFig. 8: (a)散射体周期性排列的透射光谱，图中上部是通过反射得到的可以看到前两个Bloch带，显示了Hofstadter蝴蝶的两个部分. (其中黑白分别对应高透射率和低透射率); (b)四个Bloch纹，隐约可以看见每一个中的Hofstadter能谱. (Image source: Kuhl et al. 1998)\n2017年9月，Google的John Martinis小组与CQT的Angelakis 小组合作发表了使用9个超导量子比特的相互作用光子，去模拟磁场中电子的2维结果9，复现出了Hofstadter蝴蝶.\nFig. 9: (a)在无量纲磁场 $b$ 的100个取值范围为 $0\\sim 1$ 时，恢复了Hofstadter蝴蝶; (b)对于每个 $b$ 值，识别出9个峰，并将它们的位置绘制为一系列彩点，点的颜色是测量特征值与数值计算特征值之差的绝对值. (Image source: Roushan et al. 2017)\n2020年，有研究小组报告了在六方氮化硼基底上制造的石墨烯器件中存在霍夫施塔特蝴蝶光谱的证据10. 在石墨烯晶格与氮化硼接近零角度，由外加磁场和大规模 moiré 结构之间的相互作用产生了蝴蝶型光谱.\nFig. 10: 采用逐步扫描门电压 $v_{g}$ 与磁场 $B$ 测量图，实验在两个特定温度下进行分段测量 (以14T作为分界线来设置T=10或250mK)，最后得到了蝴蝶型光谱. (Image source: Barrier et al. 2020)\n结语 从霜花到海岸线，自然界中分形无处不在，近现代物理学中同样不断能够找到分形的影子. 这一种有趣的量子分形现象——Hofstadter蝴蝶，从它与整数量子霍尔效应的联系出发，最近火热的转角石墨烯体系、分数陈绝缘体等与其相关密切. 从现在的眼光来看，Hofstadter\u0026rsquo;s Butterfly本身就是朗道能级的二维格点版本. 本文也讨论了对于二维晶格的体系，加上对应的磁通，提取系统本征能量后便可以得到一个美丽的分形图案. 但若要在实验上得以实现，需要极大的磁通，最后也举例了最新的实验观测情况. 实际上，Hofstadter蝴蝶中与之相关的物理现象有很多，例如分数量子霍尔效应也与之有重要的联系.\nReferences Mandelbrot, Benoit B. \u0026ldquo;Les objets fractals: forme, hasard et dimension.\u0026rdquo; (1975).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGao, Chao, Hui Zhai, and Zhe-Yu Shi. \u0026ldquo;Dynamical fractal in quantum gases with discrete scaling symmetry.\u0026rdquo; Physical Review Letters 122.23 (2019): 230402.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKlitzing, K. V., Gerhard Dorda, and Michael Pepper. \u0026ldquo;New method for high-accuracy determination of the fine-structure constant based on quantized Hall resistance.\u0026rdquo; Physical review letters 45.6 (1980): 494.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSatija, Indubala. \u0026ldquo;Pythagorean triplets, integral apollonians and the Hofstadter butterfly.\u0026rdquo; arXiv preprint arXiv:1802.04585 (2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJános K. Asbóth, László Oroszlány, András Pályi. \u0026ldquo;A Short Course on Topological Insulators: Band Structure and Edge States in One and Two Dimensions.\u0026rdquo; Springer International Publishing, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCage, Marvin E., et al. \u0026ldquo;The quantum Hall effect.\u0026rdquo; Springer Science \u0026amp; Business Media, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnalytis, James G., Stephen J. Blundell, and Arzhang Ardavan. \u0026ldquo;Landau levels, molecular orbitals, and the Hofstadter butterfly in finite systems.\u0026rdquo; American Journal of Physics 72.5 (2004): 613-618.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKuhl, U., and H-J. Stöckmann. \u0026ldquo;Microwave realization of the Hofstadter butterfly.\u0026rdquo; Physical review letters 80.15 (1998): 3232.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRoushan, Pedram, et al. \u0026ldquo;Spectroscopic signatures of localization with interacting photons in superconducting qubits.\u0026rdquo; Science 358.6367 (2017): 1175-1179.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrier, Julien, et al. \u0026ldquo;Long-range ballistic transport of Brown-Zak fermions in graphene superlattices.\u0026rdquo; Nature Communications 11.1 (2020): 5756.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://kairoswang.github.io/posts/2023-07-01-hofstadter_butterfly/","summary":"从霜花到海岸线，自然界中分形无处不在，近现代物理学中同样不断能够找到分形的影子. 本文回顾了通过类比 Weierstrass 函数，可以从量子气体中寻求到相应的动力学","title":"量子分形世界-Hofstadter蝴蝶"}]