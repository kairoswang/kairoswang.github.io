<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Deep Reinforcement Learning on Love and Share</title>
    <link>https://kairoswang.github.io/tags/deep-reinforcement-learning/</link>
    <description>Recent content in Deep Reinforcement Learning on Love and Share</description>
    <generator>Hugo -- 0.126.1</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Feb 2024 17:47:42 +0800</lastBuildDate>
    <atom:link href="https://kairoswang.github.io/tags/deep-reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction to reinforcement learning</title>
      <link>https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/</link>
      <pubDate>Wed, 28 Feb 2024 17:47:42 +0800</pubDate>
      <guid>https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/</guid>
      <description>Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.</description>
      <content:encoded><![CDATA[<h3 id="overview-of-reinforcement-learning">Overview of Reinforcement Learning</h3>
<p>Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.</p>
<h3 id="definition-of-deep-reinforcement-learning">Definition of Deep Reinforcement Learning</h3>
<h4 id="something-about-definitions--notation">Something about definitions &amp; notation</h4>
<p>Here, I recommend this textbook named <a href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"><em>Mathematical foundation of reinforcement learning</em></a> for studying the fundamental concepts related to classical reinforcement learning.</p>
<img src="./Fig_1.png" style="zoom: 100%; display: block; margin: 0 auto;" width="500px"/>
<ul>
<li>$\mathbf{s}_{t}$ - state</li>
<li>$\mathbf{o}_{t}$ - observation</li>
<li>$\mathbf{a}_{t}$ - action</li>
<li>$\pi _{\theta}({\mathbf{a} _{t}} | \mathbf{o} _{t})$ - policy (partially observed) - Instead, it receives an observation $\mathbf{o} _{t}$, which might be a partial or noisy view of the true state. This policy defines the probability of agent taking action $\mathbf{a} _{t}$ at​ the given observation.</li>
<li>$\pi _{\theta}({\mathbf{a} _{t}} | \mathbf{s} _{t})$ - policy (fully observed) - In a fully observed environment, the agent has access to complete state $\mathbf{s} _{t}$ of environment. The policy $\pi _{\theta}(\mathbf{a} _t | \mathbf{s} _t)$ defines the probability of agent taking action $\mathbf{a}_t$.</li>
</ul>
<p>Among all of them, important definitions to know are the state which we denote $\mathbf{s}_{t}$, the observation $\mathbf{o} _{t}$ and the action $\mathbf{a} _{t}$. Then, the observation and state could be related to one another by the following graphical model where the edge between observations and actions is policy, and state satisfies the Markov property.</p>
<img src="./Fig_2.png" style="zoom: 100%; display: block; margin: 0 auto;" width="500px"/>
<p>We&rsquo;ll start with something called Markov chain, which is named after <em>Andrei Markov</em> who was a mathematician pioneered the study of stochastic processes. The Markov chain has a very simple definition，it consists of just two things, a set of states s and a transition function, which means that the state at time $t+1$ is independent of the state at time $t-1$, one condition on the current state $\mathbf{s}_{t}$.</p>
<p>Markov chain:</p>
<ul>
<li>$\mathcal{M} = {\mathcal{S}, \mathcal{T}}$</li>
<li>$\mathcal{S}$ - state space, state $s \in \mathcal{S}$ (discrete or continuous)</li>
<li>$\mathcal{T}$ - transition operator $p(s_{t+1}|s_{t})$, a sort of linear operator, it can also be referred to as a transition probability or a dynamics function.</li>
</ul>
<img src="./Fig_3.png" style="zoom: 100%; display: block; margin: 0 auto;" width="500px"/>
<p>It sounds like a little weird, why is it refferred to an operator?</p>
<blockquote>
<p>Answer: This operator emphasizes how the Markov chain&rsquo;s dynamics are governed: it takes the current distribution of states and produces the next distribution, much like how a function or matrix transforms inputs to outputs in mathematics. It will help us capture the essence of how states evolve over time in the Markov process.</p>
</blockquote>
<blockquote>
<p>If we represent the probabilities of each state at time step $t$ as a vector, we could call it $\mu_{t,i}=p(s_{t}=i)$. Let&rsquo;s say we have $n$ states, each with its own probability distribution represented by a probability vector $\vec{\mu _{t}}$, where $t$ represents the time step. Then, the transition probabilities as a matrix, where the $ij$-th entry is the probability of going into state $i$ if you are currently in the state $j$, the corresponding formula is
$\mathcal{T} _{i,j}= p(s _{t+1} = i | s _{t}=j)$. Now, we could express the vector of state probabilities at the next time step $\vec{\mu _{t+1}} = \mathcal{T}\vec{\mu _{t}}$.</p>
</blockquote>
<p>However, the Markov chain itself doesn&rsquo;t allow us to specify a decision, as it lacks the notion of actions. In order to go towards the notion of actions, we have to turn the Markov chain into a Markov decision process (MDP).</p>
<p>Markov decision process:</p>
<ul>
<li>$\mathcal{M} = {\mathcal{S}, \mathcal{A}, \mathcal{T}, r}$</li>
<li>$\mathcal{S}$ - state space, state $s \in \mathcal{S}$ (discrete or continuous)</li>
<li>$\mathcal{A}$ - action space, actions $a \in \mathcal{A}$ (discrete or continuous)</li>
<li>$\mathcal{T}$ - transition operator, it&rsquo;s not a matrix any more, but a tensor! Because it includes next state, current state, and current action.</li>
<li>$r$ - reward function, $r: \mathcal{S}\times \mathcal{A} = \lbrace (s,a)| s \in \mathcal{S}, a \in \mathcal{A} \rbrace \rightarrow \mathbb{R}$, the reward function is a mapping from the <em>Cartesian</em> product of the state and action space into real valued numbers.</li>
</ul>
<img src="./Fig_4.png" style="zoom: 100%; display: block; margin: 0 auto;" width="400px"/>
<p>Some useful tricks we mentioned earlier can still be applied here, let $\mu_{t,j} = p(s_{t}=j)$, and we could have another vector $\xi_{t,k}=p(a_{t}=k)$ that will denote the probability of taking some action. Also, transition operator could be written as a tensor, so $\mathcal{T} _{i,j,k}= p(s _{t+1}=i | s _{t}=j,a _{t}=k)$ is the probability of entering state $i$ if you&rsquo;re in state $j$ and take action $k$. The vector of state probabilities at the next time step will be a little bit complex,</p>
<div>
$$
\mu_{t+1,i}=\sum_{j,k}\mathcal{T}_{i,j,k}\mu_{t,j}\xi_{t,k}
$$
</div>
(P.S. the reason why we don't use  $\vec{}$  is that $\mu_{t+1,i}$ as a tensor, not matrix.)
<p>Before we go to the next part, we&rsquo;d like to extend this Markov decision process definition, which will allow us to bring in the notion of observations. So a partially observed Markov decision process further augments the definition within two additional objects, observation space $\mathcal{O}$ and emission probability $\mathcal{E}$.</p>
<p>Partially observed Markov decision process:</p>
<ul>
<li>$\mathcal{M} = {\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{E}, r}$</li>
<li>$\mathcal{S}$ - state space, state $s \in \mathcal{S}$ (discrete or continuous)</li>
<li>$\mathcal{A}$ - action space, actions $a \in \mathcal{A}$ (discrete or continuous)</li>
<li>$\mathcal{O}$ - observation space, observations $o \in \mathcal{O}$ (discrete or continuous)</li>
<li>$\mathcal{T}$ - transition operator, a tensor!</li>
<li>$\mathcal{E}$ - emission probability $p(o_{t}|s_{t})$</li>
<li>$r$ - reward function, $r: \mathcal{S}\times \mathcal{A} =\lbrace (s,a)| s \in \mathcal{S}, a \in \mathcal{A}\rbrace \rightarrow \mathbb{R}$</li>
</ul>
<h4 id="goal-of-reinforcement-learning">Goal of reinforcement learning</h4>
<p>We will talk about partially observed later, for now let&rsquo;s just say our policy is conditioned on $s$, and $\theta$ corresponds to the parameters of the policy. If policy is a kind of neural network, we could find that $\theta$ denotes the parameters of this deep neural net. The state input policy and action go into the transition probability $p(\mathbf{s}^{\prime}|\mathbf{s}, \mathbf{a})$, which produces the next state.</p>
<img src="./Fig_5.png" style="zoom: 100%; display: block; margin: 0 auto;" width="550px"/>
<p>In this process, we could write down a probability distribution over trajectories, which are sequences of states and actions by using chain rules.</p>
<div>
$$
\begin{equation}
			\underbrace{ p_{\theta}(\mathbf{s}_{1},\mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T} ) }_{ p_{\theta}(\tau) } = p(\mathbf{s}_{1}) \prod^{T}_{t=1}\underbrace{ \pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})p(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t}) }_{ \text{Markov chain on} (\mathbf{s},\mathbf{a}) } 
\end{equation} 
$$
</div>
For now we assume that our control problem has a finite horizon, implying that the decision-making task is limited to a fixed time step $T$ and then ends. We could factorize it by using the chain rule in terms of probability distribution that we've already defined. For notational brevity, sometimes $p_{\theta}(\mathbf{s}_{1},\mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T} )$ will be rewritten by $p_{\theta}(\tau)$. Having define the distribution, we could actually define an objective as an expected value under the trajectory distribution for reinforcement learning tasks. (P.S. this equation could be used in finite horizon situation)
$$
\begin{equation}
	\theta^{*} = \arg \max_{\theta} E_{\tau \sim p_{\theta}(\tau)}  \left[ \sum_{t}r(\mathbf{s}_{t},\mathbf{a}_{t}) \right] 
\end{equation}
$$
We would like to find the paraments $\theta$, which is our goal. By doing this, we aim to maxmize the expected value of the sum of rewards over the trajectory. 
<p>Generally, $\pi_{\theta}(\mathbf{a} _{t}|\mathbf{\mathbf{s} _{t}})$ given $\mathbf{s} _{t}$ allows us to get a distribution of our actions condition on states. What we can do is to group state and action together into a kind of augmented state. And now the augments states actually form a Markov chain as the following figure. We could write down the enhanced transition operator (personal nick name for this operator) in this augmented Markov chain.</p>
<div>
$$
	p((\mathbf{s}_{t+1},\mathbf{a}_{t+1})|(\mathbf{s}_{t},\mathbf{a}_{t})) = p(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})\pi_{\theta}(\mathbf{a}_{t+1},\mathbf{s}_{t+1})
$$
</div>
<img src="./Fig_6.png" style="zoom: 100%; display: block; margin: 0 auto;" width="400px"/>
<p>Next we could do is to write the objective by linearity of expectation as the sum over time of the expected values under the state action marginal $p_{\theta}(\mathbf{s} _{t}, \mathbf{a} _{t})$ in this Markov chain, cause this transition operator is product of transition and policy with good linear mathematical structure.</p>
<div>
$$
\begin{equation}
	 \theta^{*} = \arg \max_{\theta}\sum^{T}_{t = 1} E_{(\mathbf{s}_{t}, \mathbf{a}_{t})\sim p_{\theta}(\mathbf{s}_{t},\mathbf{a}_{t})}[r(\mathbf{s}_{t}, \mathbf{a}_{t})]
\end{equation}
$$
</div>
This formulation simplifies the computation by focusing solely on the expectation of rewards at each time step, rather than considering the expectation over the entire trajectory.
<h4 id="infinite-horizon-case">Infinite horizon case</h4>
<p>What if $T \rightarrow \infty$ ?</p>
<blockquote>
<p>First, if that happens our objective might become not so clear. Please imagine that you have a sum of infinite positive numbers rewards, which is infinity. We need to make the objective finite, using discount will solve this problem，or choose a mathematical operation divide by $T$ so that the sum of the expectations of the marginals becomes dominated by the stationary distribution terms.</p>
</blockquote>
<p>Now, we could ask a more specific question, does $p(\mathbf{s} _{t},\mathbf{a} _{t})$ converge to a stationary distribution?</p>
<blockquote>
<p>If this is possible, that means we could write the stationary distribution and it must obey this equation, $\mu=\mathcal{T}\mu$. We could prove this conclusion under a few technical assumptions namely ergoicity and chain being aperiodic. Ergoicity means every state can be reached from every other state with non-zero probability, it prevents a situation where if you start in one part of the MDP, you might never reach another one. It dives us into characteristic equation in linear algebra to solve</p>
<div>$$(\mathcal{T}-\mathbf{I})\mu=0$$</div>
We could find $\mu$ by finding the eigenvector with eigenvalue one for the matrix defined by $t$. And $\mu$ is eigenvector of $\mathcal{T}$ with eigenvalue $1$, it always exists under ergodic and aperiodic assumption.
</blockquote>
<p>When we consider the average reward case divided by $T$, the limit as $t$ approaches infinity will be the expected value of the reward. Formally, this is expressed as</p>
<div>
$$
\begin{equation}
    \begin{aligned}
        \theta^{*} &= \arg \max_{\theta} \frac{1}{T} \sum_{t=1}^{T} E_{(\mathbf{s}_{t}, \mathbf{a}_{t}) \sim p_{\theta}(\mathbf{s}, \mathbf{a})} [r(\mathbf{s}_{t}, \mathbf{a}_{t})] \rightarrow \color{red}{(E_{(\mathbf{s}, \mathbf{a}) \sim p_{\theta}} [r(\mathbf{s}, \mathbf{a})])}
    \end{aligned}
\end{equation}
$$
</div>
<p>In simpler way, as $T$ (the number of time steps) becomes very large, the average reward converges to the expected value of the reward given the policy $p_{\theta}$.</p>
<h3 id="algorithm">Algorithm</h3>
<p>The reinforcement learning algorithms presented in this discussion, all of them would have more or less the same anatomy. The first part of our workflow is to generate samples, reinforcement learning is about learning through trial and error. Simply speaking, samples are the paths we have taken through the environment to reach our current position. Imagine that we have collected some samples. The next step is to find a suitable model for the dynamics in our model-based reinforcement learning algorithm. Then we turn into the final part, which is where you actually change your policy to make it better.</p>
<div>
$$
	\mathcal{J}(\theta) = E_{\pi}\left[ \sum_{t}r_{t} \right] \approx \frac{1}{N}\sum^{N}_{i = 1}\sum_{t}r_{t}^{i}
$$
</div>
<p>$$
\theta \leftarrow \theta +\alpha \nabla_{\theta}\mathcal{J}(\theta)
$$</p>
<p>comming soon&hellip;</p>
<h4 id="value-functions">Value functions</h4>
<p>comming soon&hellip;</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
