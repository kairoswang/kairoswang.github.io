<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to reinforcement learning | Love and Share</title>
<meta name="keywords" content="Deep reinforcement learning">
<meta name="description" content="Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.">
<meta name="author" content="Kairos">
<link rel="canonical" href="https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fa3a950dc53e39fd52e88f8aa4fe3534775854957275bcd2b38a217fb10f2c14.css" integrity="sha256-&#43;jqVDcU&#43;Of1S6I&#43;KpP41NHdYVJVydbzSs4ohf7EPLBQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://kairoswang.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://kairoswang.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://kairoswang.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://kairoswang.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://kairoswang.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>






<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [['$$','$$']], 
            inlineMath: [['$','$']],
        },
        TeX: {equationNumbers: {autoNumber: "AMS"}},
    });
</script>






  

<meta property="og:title" content="Introduction to reinforcement learning" />
<meta property="og:description" content="Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-28T17:47:42+08:00" />
<meta property="article:modified_time" content="2024-02-28T17:47:42+08:00" /><meta property="og:site_name" content="üëã Welcome" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to reinforcement learning"/>
<meta name="twitter:description" content="Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://kairoswang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Introduction to reinforcement learning",
      "item": "https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to reinforcement learning",
  "name": "Introduction to reinforcement learning",
  "description": "Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.",
  "keywords": [
    "Deep reinforcement learning"
  ],
  "articleBody": "Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.\nDefinition of Deep Reinforcement Learning Something about definitions \u0026 notation Here, I recommend this textbook named Mathematical foundation of reinforcement learning for studying the fundamental concepts related to classical reinforcement learning.\n$\\mathbf{s}_{t}$ - state $\\mathbf{o}_{t}$ - observation $\\mathbf{a}_{t}$ - action $\\pi _{\\theta}({\\mathbf{a} _{t}} | \\mathbf{o} _{t})$ - policy (partially observed) - Instead, it receives an observation $\\mathbf{o} _{t}$, which might be a partial or noisy view of the true state. This policy defines the probability of agent taking action $\\mathbf{a} _{t}$ at‚Äã the given observation. $\\pi _{\\theta}({\\mathbf{a} _{t}} | \\mathbf{s} _{t})$ - policy (fully observed) - In a fully observed environment, the agent has access to complete state $\\mathbf{s} _{t}$ of environment. The policy $\\pi _{\\theta}(\\mathbf{a} _t | \\mathbf{s} _t)$ defines the probability of agent taking action $\\mathbf{a}_t$. Among all of them, important definitions to know are the state which we denote $\\mathbf{s}_{t}$, the observation $\\mathbf{o} _{t}$ and the action $\\mathbf{a} _{t}$. Then, the observation and state could be related to one another by the following graphical model where the edge between observations and actions is policy, and state satisfies the Markov property.\nWe‚Äôll start with something called Markov chain, which is named after Andrei Markov who was a mathematician pioneered the study of stochastic processes. The Markov chain has a very simple definitionÔºåit consists of just two things, a set of states s and a transition function, which means that the state at time $t+1$ is independent of the state at time $t-1$, one condition on the current state $\\mathbf{s}_{t}$.\nMarkov chain:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{T}}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{T}$ - transition operator $p(s_{t+1}|s_{t})$, a sort of linear operator, it can also be referred to as a transition probability or a dynamics function. It sounds like a little weird, why is it refferred to an operator?\nAnswer: This operator emphasizes how the Markov chain‚Äôs dynamics are governed: it takes the current distribution of states and produces the next distribution, much like how a function or matrix transforms inputs to outputs in mathematics. It will help us capture the essence of how states evolve over time in the Markov process.\nIf we represent the probabilities of each state at time step $t$ as a vector, we could call it $\\mu_{t,i}=p(s_{t}=i)$. Let‚Äôs say we have $n$ states, each with its own probability distribution represented by a probability vector $\\vec{\\mu _{t}}$, where $t$ represents the time step. Then, the transition probabilities as a matrix, where the $ij$-th entry is the probability of going into state $i$ if you are currently in the state $j$, the corresponding formula is $\\mathcal{T} _{i,j}= p(s _{t+1} = i | s _{t}=j)$. Now, we could express the vector of state probabilities at the next time step $\\vec{\\mu _{t+1}} = \\mathcal{T}\\vec{\\mu _{t}}$.\nHowever, the Markov chain itself doesn‚Äôt allow us to specify a decision, as it lacks the notion of actions. In order to go towards the notion of actions, we have to turn the Markov chain into a Markov decision process (MDP).\nMarkov decision process:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, r}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{A}$ - action space, actions $a \\in \\mathcal{A}$ (discrete or continuous) $\\mathcal{T}$ - transition operator, it‚Äôs not a matrix any more, but a tensor! Because it includes next state, current state, and current action. $r$ - reward function, $r: \\mathcal{S}\\times \\mathcal{A} = \\lbrace (s,a)| s \\in \\mathcal{S}, a \\in \\mathcal{A} \\rbrace \\rightarrow \\mathbb{R}$, the reward function is a mapping from the Cartesian product of the state and action space into real valued numbers. Some useful tricks we mentioned earlier can still be applied here, let $\\mu_{t,j} = p(s_{t}=j)$, and we could have another vector $\\xi_{t,k}=p(a_{t}=k)$ that will denote the probability of taking some action. Also, transition operator could be written as a tensor, so $\\mathcal{T} _{i,j,k}= p(s _{t+1}=i | s _{t}=j,a _{t}=k)$ is the probability of entering state $i$ if you‚Äôre in state $j$ and take action $k$. The vector of state probabilities at the next time step will be a little bit complex,\n$$ \\mu_{t+1,i}=\\sum_{j,k}\\mathcal{T}_{i,j,k}\\mu_{t,j}\\xi_{t,k} $$ (P.S. the reason why we don't use $\\vec{}$ is that $\\mu_{t+1,i}$ as a tensor, not matrix.) Before we go to the next part, we‚Äôd like to extend this Markov decision process definition, which will allow us to bring in the notion of observations. So a partially observed Markov decision process further augments the definition within two additional objects, observation space $\\mathcal{O}$ and emission probability $\\mathcal{E}$.\nPartially observed Markov decision process:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{O}, \\mathcal{T}, \\mathcal{E}, r}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{A}$ - action space, actions $a \\in \\mathcal{A}$ (discrete or continuous) $\\mathcal{O}$ - observation space, observations $o \\in \\mathcal{O}$ (discrete or continuous) $\\mathcal{T}$ - transition operator, a tensor! $\\mathcal{E}$ - emission probability $p(o_{t}|s_{t})$ $r$ - reward function, $r: \\mathcal{S}\\times \\mathcal{A} =\\lbrace (s,a)| s \\in \\mathcal{S}, a \\in \\mathcal{A}\\rbrace \\rightarrow \\mathbb{R}$ Goal of reinforcement learning We will talk about partially observed later, for now let‚Äôs just say our policy is conditioned on $s$, and $\\theta$ corresponds to the parameters of the policy. If policy is a kind of neural network, we could find that $\\theta$ denotes the parameters of this deep neural net. The state input policy and action go into the transition probability $p(\\mathbf{s}^{\\prime}|\\mathbf{s}, \\mathbf{a})$, which produces the next state.\nIn this process, we could write down a probability distribution over trajectories, which are sequences of states and actions by using chain rules.\n$$ \\begin{equation} \\underbrace{ p_{\\theta}(\\mathbf{s}_{1},\\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T} ) }_{ p_{\\theta}(\\tau) } = p(\\mathbf{s}_{1}) \\prod^{T}_{t=1}\\underbrace{ \\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})p(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t}) }_{ \\text{Markov chain on} (\\mathbf{s},\\mathbf{a}) } \\end{equation} $$ For now we assume that our control problem has a finite horizon, implying that the decision-making task is limited to a fixed time step $T$ and then ends. We could factorize it by using the chain rule in terms of probability distribution that we've already defined. For notational brevity, sometimes $p_{\\theta}(\\mathbf{s}_{1},\\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T} )$ will be rewritten by $p_{\\theta}(\\tau)$. Having define the distribution, we could actually define an objective as an expected value under the trajectory distribution for reinforcement learning tasks. (P.S. this equation could be used in finite horizon situation) $$ \\begin{equation} \\theta^{*} = \\arg \\max_{\\theta} E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\sum_{t}r(\\mathbf{s}_{t},\\mathbf{a}_{t}) \\right] \\end{equation} $$ We would like to find the paraments $\\theta$, which is our goal. By doing this, we aim to maxmize the expected value of the sum of rewards over the trajectory. Generally, $\\pi_{\\theta}(\\mathbf{a} _{t}|\\mathbf{\\mathbf{s} _{t}})$ given $\\mathbf{s} _{t}$ allows us to get a distribution of our actions condition on states. What we can do is to group state and action together into a kind of augmented state. And now the augments states actually form a Markov chain as the following figure. We could write down the enhanced transition operator (personal nick name for this operator) in this augmented Markov chain.\n$$ p((\\mathbf{s}_{t+1},\\mathbf{a}_{t+1})|(\\mathbf{s}_{t},\\mathbf{a}_{t})) = p(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})\\pi_{\\theta}(\\mathbf{a}_{t+1},\\mathbf{s}_{t+1}) $$ Next we could do is to write the objective by linearity of expectation as the sum over time of the expected values under the state action marginal $p_{\\theta}(\\mathbf{s} _{t}, \\mathbf{a} _{t})$ in this Markov chain, cause this transition operator is product of transition and policy with good linear mathematical structure.\n$$ \\begin{equation} \\theta^{*} = \\arg \\max_{\\theta}\\sum^{T}_{t = 1} E_{(\\mathbf{s}_{t}, \\mathbf{a}_{t})\\sim p_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})}[r(\\mathbf{s}_{t}, \\mathbf{a}_{t})] \\end{equation} $$ This formulation simplifies the computation by focusing solely on the expectation of rewards at each time step, rather than considering the expectation over the entire trajectory. Infinite horizon case What if $T \\rightarrow \\infty$ ?\nFirst, if that happens our objective might become not so clear. Please imagine that you have a sum of infinite positive numbers rewards, which is infinity. We need to make the objective finite, using discount will solve this problemÔºåor choose a mathematical operation divide by $T$ so that the sum of the expectations of the marginals becomes dominated by the stationary distribution terms.\nNow, we could ask a more specific question, does $p(\\mathbf{s} _{t},\\mathbf{a} _{t})$ converge to a stationary distribution?\nIf this is possible, that means we could write the stationary distribution and it must obey this equation, $\\mu=\\mathcal{T}\\mu$. We could prove this conclusion under a few technical assumptions namely ergoicity and chain being aperiodic. Ergoicity means every state can be reached from every other state with non-zero probability, it prevents a situation where if you start in one part of the MDP, you might never reach another one. It dives us into characteristic equation in linear algebra to solve\n$$(\\mathcal{T}-\\mathbf{I})\\mu=0$$ We could find $\\mu$ by finding the eigenvector with eigenvalue one for the matrix defined by $t$. And $\\mu$ is eigenvector of $\\mathcal{T}$ with eigenvalue $1$, it always exists under ergodic and aperiodic assumption. When we consider the average reward case divided by $T$, the limit as $t$ approaches infinity will be the expected value of the reward. Formally, this is expressed as\n$$ \\begin{equation} \\begin{aligned} \\theta^{*} \u0026= \\arg \\max_{\\theta} \\frac{1}{T} \\sum_{t=1}^{T} E_{(\\mathbf{s}_{t}, \\mathbf{a}_{t}) \\sim p_{\\theta}(\\mathbf{s}, \\mathbf{a})} [r(\\mathbf{s}_{t}, \\mathbf{a}_{t})] \\rightarrow \\color{red}{(E_{(\\mathbf{s}, \\mathbf{a}) \\sim p_{\\theta}} [r(\\mathbf{s}, \\mathbf{a})])} \\end{aligned} \\end{equation} $$ In simpler way, as $T$ (the number of time steps) becomes very large, the average reward converges to the expected value of the reward given the policy $p_{\\theta}$.\nAlgorithm The reinforcement learning algorithms presented in this discussion, all of them would have more or less the same anatomy. The first part of our workflow is to generate samples, reinforcement learning is about learning through trial and error. Simply speaking, samples are the paths we have taken through the environment to reach our current position. Imagine that we have collected some samples. The next step is to find a suitable model for the dynamics in our model-based reinforcement learning algorithm. Then we turn into the final part, which is where you actually change your policy to make it better.\n$$ \\mathcal{J}(\\theta) = E_{\\pi}\\left[ \\sum_{t}r_{t} \\right] \\approx \\frac{1}{N}\\sum^{N}_{i = 1}\\sum_{t}r_{t}^{i} $$ $$ \\theta \\leftarrow \\theta +\\alpha \\nabla_{\\theta}\\mathcal{J}(\\theta) $$\ncomming soon‚Ä¶\nValue functions comming soon‚Ä¶\n",
  "wordCount" : "1717",
  "inLanguage": "en",
  "datePublished": "2024-02-28T17:47:42+08:00",
  "dateModified": "2024-02-28T17:47:42+08:00",
  "author":{
    "@type": "Person",
    "name": "Kairos"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kairoswang.github.io/posts/2024-02-28-lec1_rl_introduction/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Love and Share",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kairoswang.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://kairoswang.github.io/" accesskey="h" title="Love and Share (Alt + H)">Love and Share</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://kairoswang.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://kairoswang.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://kairoswang.github.io/search/" title="üîç (Alt &#43; /)" accesskey=/>
                    <span>üîç</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post" autonumbering>
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Introduction to reinforcement learning
    </h1>
    <div class="post-meta"><span title='2024-02-28 17:47:42 +0800 CST'>2024-02-28</span>&nbsp;¬∑&nbsp;9 min&nbsp;¬∑&nbsp;Kairos

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#overview-of-reinforcement-learning" aria-label="Overview of Reinforcement Learning">Overview of Reinforcement Learning</a></li>
                    <li>
                        <a href="#definition-of-deep-reinforcement-learning" aria-label="Definition of Deep Reinforcement Learning">Definition of Deep Reinforcement Learning</a><ul>
                            
                    <li>
                        <a href="#something-about-definitions--notation" aria-label="Something about definitions &amp;amp; notation">Something about definitions &amp; notation</a></li>
                    <li>
                        <a href="#goal-of-reinforcement-learning" aria-label="Goal of reinforcement learning">Goal of reinforcement learning</a></li>
                    <li>
                        <a href="#infinite-horizon-case" aria-label="Infinite horizon case">Infinite horizon case</a></li></ul>
                    </li>
                    <li>
                        <a href="#algorithm" aria-label="Algorithm">Algorithm</a><ul>
                            
                    <li>
                        <a href="#value-functions" aria-label="Value functions">Value functions</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>
  <div class="post-content"><h3 id="overview-of-reinforcement-learning">Overview of Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#overview-of-reinforcement-learning">#</a></h3>
<p>Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.</p>
<h3 id="definition-of-deep-reinforcement-learning">Definition of Deep Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#definition-of-deep-reinforcement-learning">#</a></h3>
<h4 id="something-about-definitions--notation">Something about definitions &amp; notation<a hidden class="anchor" aria-hidden="true" href="#something-about-definitions--notation">#</a></h4>
<p>Here, I recommend this textbook named <a href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"><em>Mathematical foundation of reinforcement learning</em></a> for studying the fundamental concepts related to classical reinforcement learning.</p>
<img src="./Fig_1.png" style="zoom: 100%; display: block; margin: 0 auto;" width="500px"/>
<ul>
<li>$\mathbf{s}_{t}$ - state</li>
<li>$\mathbf{o}_{t}$ - observation</li>
<li>$\mathbf{a}_{t}$ - action</li>
<li>$\pi _{\theta}({\mathbf{a} _{t}} | \mathbf{o} _{t})$ - policy (partially observed) - Instead, it receives an observation $\mathbf{o} _{t}$, which might be a partial or noisy view of the true state. This policy defines the probability of agent taking action $\mathbf{a} _{t}$ at‚Äã the given observation.</li>
<li>$\pi _{\theta}({\mathbf{a} _{t}} | \mathbf{s} _{t})$ - policy (fully observed) - In a fully observed environment, the agent has access to complete state $\mathbf{s} _{t}$ of environment. The policy $\pi _{\theta}(\mathbf{a} _t | \mathbf{s} _t)$ defines the probability of agent taking action $\mathbf{a}_t$.</li>
</ul>
<p>Among all of them, important definitions to know are the state which we denote $\mathbf{s}_{t}$, the observation $\mathbf{o} _{t}$ and the action $\mathbf{a} _{t}$. Then, the observation and state could be related to one another by the following graphical model where the edge between observations and actions is policy, and state satisfies the Markov property.</p>
<img src="./Fig_2.png" style="zoom: 100%; display: block; margin: 0 auto;" width="500px"/>
<p>We&rsquo;ll start with something called Markov chain, which is named after <em>Andrei Markov</em> who was a mathematician pioneered the study of stochastic processes. The Markov chain has a very simple definitionÔºåit consists of just two things, a set of states s and a transition function, which means that the state at time $t+1$ is independent of the state at time $t-1$, one condition on the current state $\mathbf{s}_{t}$.</p>
<p>Markov chain:</p>
<ul>
<li>$\mathcal{M} = {\mathcal{S}, \mathcal{T}}$</li>
<li>$\mathcal{S}$ - state space, state $s \in \mathcal{S}$ (discrete or continuous)</li>
<li>$\mathcal{T}$ - transition operator $p(s_{t+1}|s_{t})$, a sort of linear operator, it can also be referred to as a transition probability or a dynamics function.</li>
</ul>
<img src="./Fig_3.png" style="zoom: 100%; display: block; margin: 0 auto;" width="500px"/>
<p>It sounds like a little weird, why is it refferred to an operator?</p>
<blockquote>
<p>Answer: This operator emphasizes how the Markov chain&rsquo;s dynamics are governed: it takes the current distribution of states and produces the next distribution, much like how a function or matrix transforms inputs to outputs in mathematics. It will help us capture the essence of how states evolve over time in the Markov process.</p>
</blockquote>
<blockquote>
<p>If we represent the probabilities of each state at time step $t$ as a vector, we could call it $\mu_{t,i}=p(s_{t}=i)$. Let&rsquo;s say we have $n$ states, each with its own probability distribution represented by a probability vector $\vec{\mu _{t}}$, where $t$ represents the time step. Then, the transition probabilities as a matrix, where the $ij$-th entry is the probability of going into state $i$ if you are currently in the state $j$, the corresponding formula is
$\mathcal{T} _{i,j}= p(s _{t+1} = i | s _{t}=j)$. Now, we could express the vector of state probabilities at the next time step $\vec{\mu _{t+1}} = \mathcal{T}\vec{\mu _{t}}$.</p>
</blockquote>
<p>However, the Markov chain itself doesn&rsquo;t allow us to specify a decision, as it lacks the notion of actions. In order to go towards the notion of actions, we have to turn the Markov chain into a Markov decision process (MDP).</p>
<p>Markov decision process:</p>
<ul>
<li>$\mathcal{M} = {\mathcal{S}, \mathcal{A}, \mathcal{T}, r}$</li>
<li>$\mathcal{S}$ - state space, state $s \in \mathcal{S}$ (discrete or continuous)</li>
<li>$\mathcal{A}$ - action space, actions $a \in \mathcal{A}$ (discrete or continuous)</li>
<li>$\mathcal{T}$ - transition operator, it&rsquo;s not a matrix any more, but a tensor! Because it includes next state, current state, and current action.</li>
<li>$r$ - reward function, $r: \mathcal{S}\times \mathcal{A} = \lbrace (s,a)| s \in \mathcal{S}, a \in \mathcal{A} \rbrace \rightarrow \mathbb{R}$, the reward function is a mapping from the <em>Cartesian</em> product of the state and action space into real valued numbers.</li>
</ul>
<img src="./Fig_4.png" style="zoom: 100%; display: block; margin: 0 auto;" width="400px"/>
<p>Some useful tricks we mentioned earlier can still be applied here, let $\mu_{t,j} = p(s_{t}=j)$, and we could have another vector $\xi_{t,k}=p(a_{t}=k)$ that will denote the probability of taking some action. Also, transition operator could be written as a tensor, so $\mathcal{T} _{i,j,k}= p(s _{t+1}=i | s _{t}=j,a _{t}=k)$ is the probability of entering state $i$ if you&rsquo;re in state $j$ and take action $k$. The vector of state probabilities at the next time step will be a little bit complex,</p>
<div>
$$
\mu_{t+1,i}=\sum_{j,k}\mathcal{T}_{i,j,k}\mu_{t,j}\xi_{t,k}
$$
</div>
(P.S. the reason why we don't use  $\vec{}$  is that $\mu_{t+1,i}$ as a tensor, not matrix.)
<p>Before we go to the next part, we&rsquo;d like to extend this Markov decision process definition, which will allow us to bring in the notion of observations. So a partially observed Markov decision process further augments the definition within two additional objects, observation space $\mathcal{O}$ and emission probability $\mathcal{E}$.</p>
<p>Partially observed Markov decision process:</p>
<ul>
<li>$\mathcal{M} = {\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{E}, r}$</li>
<li>$\mathcal{S}$ - state space, state $s \in \mathcal{S}$ (discrete or continuous)</li>
<li>$\mathcal{A}$ - action space, actions $a \in \mathcal{A}$ (discrete or continuous)</li>
<li>$\mathcal{O}$ - observation space, observations $o \in \mathcal{O}$ (discrete or continuous)</li>
<li>$\mathcal{T}$ - transition operator, a tensor!</li>
<li>$\mathcal{E}$ - emission probability $p(o_{t}|s_{t})$</li>
<li>$r$ - reward function, $r: \mathcal{S}\times \mathcal{A} =\lbrace (s,a)| s \in \mathcal{S}, a \in \mathcal{A}\rbrace \rightarrow \mathbb{R}$</li>
</ul>
<h4 id="goal-of-reinforcement-learning">Goal of reinforcement learning<a hidden class="anchor" aria-hidden="true" href="#goal-of-reinforcement-learning">#</a></h4>
<p>We will talk about partially observed later, for now let&rsquo;s just say our policy is conditioned on $s$, and $\theta$ corresponds to the parameters of the policy. If policy is a kind of neural network, we could find that $\theta$ denotes the parameters of this deep neural net. The state input policy and action go into the transition probability $p(\mathbf{s}^{\prime}|\mathbf{s}, \mathbf{a})$, which produces the next state.</p>
<img src="./Fig_5.png" style="zoom: 100%; display: block; margin: 0 auto;" width="550px"/>
<p>In this process, we could write down a probability distribution over trajectories, which are sequences of states and actions by using chain rules.</p>
<div>
$$
\begin{equation}
			\underbrace{ p_{\theta}(\mathbf{s}_{1},\mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T} ) }_{ p_{\theta}(\tau) } = p(\mathbf{s}_{1}) \prod^{T}_{t=1}\underbrace{ \pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})p(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t}) }_{ \text{Markov chain on} (\mathbf{s},\mathbf{a}) } 
\end{equation} 
$$
</div>
For now we assume that our control problem has a finite horizon, implying that the decision-making task is limited to a fixed time step $T$ and then ends. We could factorize it by using the chain rule in terms of probability distribution that we've already defined. For notational brevity, sometimes $p_{\theta}(\mathbf{s}_{1},\mathbf{a}_{1}, \cdots, \mathbf{s}_{T}, \mathbf{a}_{T} )$ will be rewritten by $p_{\theta}(\tau)$. Having define the distribution, we could actually define an objective as an expected value under the trajectory distribution for reinforcement learning tasks. (P.S. this equation could be used in finite horizon situation)
$$
\begin{equation}
	\theta^{*} = \arg \max_{\theta} E_{\tau \sim p_{\theta}(\tau)}  \left[ \sum_{t}r(\mathbf{s}_{t},\mathbf{a}_{t}) \right] 
\end{equation}
$$
We would like to find the paraments $\theta$, which is our goal. By doing this, we aim to maxmize the expected value of the sum of rewards over the trajectory. 
<p>Generally, $\pi_{\theta}(\mathbf{a} _{t}|\mathbf{\mathbf{s} _{t}})$ given $\mathbf{s} _{t}$ allows us to get a distribution of our actions condition on states. What we can do is to group state and action together into a kind of augmented state. And now the augments states actually form a Markov chain as the following figure. We could write down the enhanced transition operator (personal nick name for this operator) in this augmented Markov chain.</p>
<div>
$$
	p((\mathbf{s}_{t+1},\mathbf{a}_{t+1})|(\mathbf{s}_{t},\mathbf{a}_{t})) = p(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})\pi_{\theta}(\mathbf{a}_{t+1},\mathbf{s}_{t+1})
$$
</div>
<img src="./Fig_6.png" style="zoom: 100%; display: block; margin: 0 auto;" width="400px"/>
<p>Next we could do is to write the objective by linearity of expectation as the sum over time of the expected values under the state action marginal $p_{\theta}(\mathbf{s} _{t}, \mathbf{a} _{t})$ in this Markov chain, cause this transition operator is product of transition and policy with good linear mathematical structure.</p>
<div>
$$
\begin{equation}
	 \theta^{*} = \arg \max_{\theta}\sum^{T}_{t = 1} E_{(\mathbf{s}_{t}, \mathbf{a}_{t})\sim p_{\theta}(\mathbf{s}_{t},\mathbf{a}_{t})}[r(\mathbf{s}_{t}, \mathbf{a}_{t})]
\end{equation}
$$
</div>
This formulation simplifies the computation by focusing solely on the expectation of rewards at each time step, rather than considering the expectation over the entire trajectory.
<h4 id="infinite-horizon-case">Infinite horizon case<a hidden class="anchor" aria-hidden="true" href="#infinite-horizon-case">#</a></h4>
<p>What if $T \rightarrow \infty$ ?</p>
<blockquote>
<p>First, if that happens our objective might become not so clear. Please imagine that you have a sum of infinite positive numbers rewards, which is infinity. We need to make the objective finite, using discount will solve this problemÔºåor choose a mathematical operation divide by $T$ so that the sum of the expectations of the marginals becomes dominated by the stationary distribution terms.</p>
</blockquote>
<p>Now, we could ask a more specific question, does $p(\mathbf{s} _{t},\mathbf{a} _{t})$ converge to a stationary distribution?</p>
<blockquote>
<p>If this is possible, that means we could write the stationary distribution and it must obey this equation, $\mu=\mathcal{T}\mu$. We could prove this conclusion under a few technical assumptions namely ergoicity and chain being aperiodic. Ergoicity means every state can be reached from every other state with non-zero probability, it prevents a situation where if you start in one part of the MDP, you might never reach another one. It dives us into characteristic equation in linear algebra to solve</p>
<div>$$(\mathcal{T}-\mathbf{I})\mu=0$$</div>
We could find $\mu$ by finding the eigenvector with eigenvalue one for the matrix defined by $t$. And $\mu$ is eigenvector of $\mathcal{T}$ with eigenvalue $1$, it always exists under ergodic and aperiodic assumption.
</blockquote>
<p>When we consider the average reward case divided by $T$, the limit as $t$ approaches infinity will be the expected value of the reward. Formally, this is expressed as</p>
<div>
$$
\begin{equation}
    \begin{aligned}
        \theta^{*} &= \arg \max_{\theta} \frac{1}{T} \sum_{t=1}^{T} E_{(\mathbf{s}_{t}, \mathbf{a}_{t}) \sim p_{\theta}(\mathbf{s}, \mathbf{a})} [r(\mathbf{s}_{t}, \mathbf{a}_{t})] \rightarrow \color{red}{(E_{(\mathbf{s}, \mathbf{a}) \sim p_{\theta}} [r(\mathbf{s}, \mathbf{a})])}
    \end{aligned}
\end{equation}
$$
</div>
<p>In simpler way, as $T$ (the number of time steps) becomes very large, the average reward converges to the expected value of the reward given the policy $p_{\theta}$.</p>
<h3 id="algorithm">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm">#</a></h3>
<p>The reinforcement learning algorithms presented in this discussion, all of them would have more or less the same anatomy. The first part of our workflow is to generate samples, reinforcement learning is about learning through trial and error. Simply speaking, samples are the paths we have taken through the environment to reach our current position. Imagine that we have collected some samples. The next step is to find a suitable model for the dynamics in our model-based reinforcement learning algorithm. Then we turn into the final part, which is where you actually change your policy to make it better.</p>
<div>
$$
	\mathcal{J}(\theta) = E_{\pi}\left[ \sum_{t}r_{t} \right] \approx \frac{1}{N}\sum^{N}_{i = 1}\sum_{t}r_{t}^{i}
$$
</div>
<p>$$
\theta \leftarrow \theta +\alpha \nabla_{\theta}\mathcal{J}(\theta)
$$</p>
<p>comming soon&hellip;</p>
<h4 id="value-functions">Value functions<a hidden class="anchor" aria-hidden="true" href="#value-functions">#</a></h4>
<p>comming soon&hellip;</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://kairoswang.github.io/tags/deep-reinforcement-learning/">Deep Reinforcement Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://kairoswang.github.io/posts/2024-06-01-belief_system/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>The challenges of scientific truth and belief systems</span>
  </a>
  <a class="next" href="https://kairoswang.github.io/posts/2023-07-09-kdv_equation/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>KdV ÊñπÁ®ãÊ±ÇËß£ÂèäÂÖ∂ËÉåÊôØ</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://kairoswang.github.io/">Love and Share</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
