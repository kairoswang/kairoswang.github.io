<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is entropy? | Love and Share</title>
<meta name="keywords" content="Physics">
<meta name="description" content="Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me‚ÄîSo, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.
Mathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics.">
<meta name="author" content="Kairos">
<link rel="canonical" href="https://kairoswang.github.io/posts/2022-06-19-what_is_entropy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fa3a950dc53e39fd52e88f8aa4fe3534775854957275bcd2b38a217fb10f2c14.css" integrity="sha256-&#43;jqVDcU&#43;Of1S6I&#43;KpP41NHdYVJVydbzSs4ohf7EPLBQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://kairoswang.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://kairoswang.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://kairoswang.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://kairoswang.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://kairoswang.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://kairoswang.github.io/posts/2022-06-19-what_is_entropy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>






<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [['$$','$$']], 
            inlineMath: [['$','$']],
        },
        TeX: {equationNumbers: {autoNumber: "AMS"}},
    });
</script>






  

<meta property="og:title" content="What is entropy?" />
<meta property="og:description" content="Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me‚ÄîSo, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.
Mathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kairoswang.github.io/posts/2022-06-19-what_is_entropy/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-29T14:39:53+08:00" />
<meta property="article:modified_time" content="2022-06-29T14:39:53+08:00" /><meta property="og:site_name" content="üëã Welcome" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="What is entropy?"/>
<meta name="twitter:description" content="Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me‚ÄîSo, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.
Mathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://kairoswang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is entropy?",
      "item": "https://kairoswang.github.io/posts/2022-06-19-what_is_entropy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is entropy?",
  "name": "What is entropy?",
  "description": "Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me‚ÄîSo, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.\nMathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics.",
  "keywords": [
    "Physics"
  ],
  "articleBody": "Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me‚ÄîSo, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.\nMathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics. Clearly and step by step:\nImagine two thermal internal interaction between two macroscopic systems $A$ and $A^{\\prime}$, we shall denote the respective energies of these systems by $E$ and $E^{\\prime}$. Moreover, the combined system $A^{(0)} \\equiv A + A^{\\prime}$ is isolated and its total energy $E^{(0)}$ is therefore constant 3.\nIt follows that the the probability $P(E)$ of finding this combineed system in a configuration where $A$ has an energy is simply proportional to the number of states $\\Omega^{(0)}(E)$. This probability could also be written as\n$$ \\begin{equation} P(E) = \\frac{\\Omega ^{(0)}(E)}{\\Omega_{\\text{tot}}}, \\end{equation} $$\nwhere $\\Omega ^{(0)}_ {\\textrm{tot}}$ denotes the total number of states accessible to $A^{(0)}$. $\\Omega ^{(0)}_ {\\textrm{tot}}$ can be obtained by summing $\\Omega^{(0)}(E)$ over all possible energies $E$ of the system $A$, which means $\\Omega ^{(0)}_{\\text{tot}}$ is constant. In symbols this can be written as\n$$ \\begin{equation} P(E) = C \\Omega^{(0)}(E), \\end{equation}\\label{eq2} $$\nwhere $C$ is a constant of proportionality independent of $E$. Indeed, suppose that $A$ has an energy $E$, and $A^{\\prime}$ has the corresponding energy known as to be $E^{\\prime} = E^{(0)}-E$. In Eq. \\eqref{eq2}, $\\Omega^{(0)}(E)$ is simply given by the product\n$$ \\begin{equation} \\Omega^{(0)}(E)=\\Omega(E) \\Omega^{\\prime}\\left(E^{(0)}-E\\right). \\end{equation} $$\nCorrespondingly, system $A$ having an energy near $E$ is simply given by\n$$ \\begin{equation} P(E)=C \\Omega(E) \\Omega^{\\prime}\\left(E^{(0)}-E\\right). \\end{equation} $$\nTo locate the position of the maximum of $P(E)$, we need to find the value when\n$$ \\begin{equation} \\frac{\\partial \\ln P}{\\partial E}=0, \\end{equation} \\label{eq5} $$\nwhere\n$$ \\ln P(E)=\\ln C+\\ln \\Omega(E)+\\ln \\Omega^{\\prime}\\left(E^{\\prime}\\right). $$\nAs what we have said before, $E^{\\prime}=E^{(0)}-E$. Hence, Eq. \\eqref{eq5} becomes\n$$ \\begin{equation} \\frac{\\partial \\ln \\Omega(E)}{\\partial E} - \\frac{\\partial \\ln \\Omega^{\\prime}\\left(E^{\\prime}\\right)}{\\partial E^{\\prime}}=0, \\end{equation} $$\nwhere we have to introduced the definition\n$$ \\begin{equation} \\beta(E) \\equiv \\frac{\\partial \\ln \\Omega}{\\partial E}. \\end{equation} $$\nThen we get $\\beta(\\tilde{E}) = \\beta^{\\prime}(\\tilde{E}^{\\prime})$, where $\\tilde{E}$ and $\\tilde{E}^{\\prime}$ denote the corresponding energies of $A$ and $A^{\\prime}$ at the maximum, By its definition, the parameter $\\beta$ has the dimensions of a reciprocal energy. It is convenient to introduce a dimensionless parameter $T$ defined by writing $\\beta = (k T)^{-1}$, where $k$ is some positive sonstant having the dimensions of energy.\nLet me explain that why is $\\beta = (k T)^{-1}$? Like what we have learnt in thermodynamics,\nentropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature.\nThe corresponding connection can be shown like this\n$$ \\begin{equation} \\frac{1}{T} = \\frac{\\partial S}{\\partial E}, \\end{equation}\\label{eq8} $$\nwe usually use this definition $S \\equiv k \\ln \\Omega$, combine it with Eq. \\eqref{eq8}, we get\n$$ \\begin{equation} \\frac{1}{T} = \\frac{k \\partial \\Omega}{\\partial E} \\rightarrow \\frac{1}{kT} \\equiv \\beta \\equiv \\frac{\\partial \\ln \\Omega}{\\partial E}. \\end{equation} $$\nThe truth is that when entropy of two subsystems reaches the maximum value of the state is the most likely state for both systems.\nReference Shannon, Claude Elwood. ‚ÄúA mathematical theory of communication.‚Äù ACM SIGMOBILE mobile computing and communications review 5.1 (2001): 3-55.¬†‚Ü©Ô∏é\nWikipedia-Entropy¬†‚Ü©Ô∏é\nReif, Frederick, and Stuart A. Rice. ‚ÄúFundamentals of statistical and thermal physics.‚Äù Physics Today 20.12 (1967): 85-87.¬†‚Ü©Ô∏é\n",
  "wordCount" : "584",
  "inLanguage": "en",
  "datePublished": "2022-06-29T14:39:53+08:00",
  "dateModified": "2022-06-29T14:39:53+08:00",
  "author":{
    "@type": "Person",
    "name": "Kairos"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kairoswang.github.io/posts/2022-06-19-what_is_entropy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Love and Share",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kairoswang.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://kairoswang.github.io/" accesskey="h" title="Love and Share (Alt + H)">Love and Share</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://kairoswang.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://kairoswang.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://kairoswang.github.io/search/" title="üîç (Alt &#43; /)" accesskey=/>
                    <span>üîç</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post" autonumbering>
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      What is entropy?
    </h1>
    <div class="post-meta"><span title='2022-06-29 14:39:53 +0800 CST'>2022-06-29</span>&nbsp;¬∑&nbsp;3 min&nbsp;¬∑&nbsp;Kairos

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                    <li>
                        <a href="#mathematical-derivation" aria-label="Mathematical derivation">Mathematical derivation</a></li>
                    <li>
                        <a href="#reference" aria-label="Reference">Reference</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>
  <div class="post-content"><h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>When it comes to deep learning, it is no doubt to encounter a concept called <strong>information entropy</strong> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. My friend asked a question for me‚ÄîSo, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Austrian physicist <em>Boltzmann</em> explained entropy as the degree of disorder or randomness in a system.</p>
<h3 id="mathematical-derivation">Mathematical derivation<a hidden class="anchor" aria-hidden="true" href="#mathematical-derivation">#</a></h3>
<p>In this blog, I will show a kind of method presented by <em>Frederick</em> in his classical book <em>Fundamentals of statistical and thermal physics</em>. Clearly and step by step:</p>
<p>Imagine two thermal internal interaction between two macroscopic systems $A$ and $A^{\prime}$, we shall denote the respective energies of these systems by $E$ and $E^{\prime}$. Moreover, the combined system $A^{(0)} \equiv A + A^{\prime}$ is isolated and its total energy $E^{(0)}$ is therefore constant <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<img src="./Fig_1.png" style="zoom: 100%; display: block; margin: 0 auto;" width="400px"/>
<p>It follows that the the probability $P(E)$ of finding this combineed system in a configuration where $A$ has an energy is simply proportional to the number of states $\Omega^{(0)}(E)$. This probability could also be written as</p>
<p>$$
\begin{equation}
P(E) = \frac{\Omega ^{(0)}(E)}{\Omega_{\text{tot}}},
\end{equation}
$$</p>
<p>where $\Omega ^{(0)}_ {\textrm{tot}}$ denotes the total number of states accessible to $A^{(0)}$. $\Omega ^{(0)}_ {\textrm{tot}}$ can be obtained by summing $\Omega^{(0)}(E)$ over all possible energies $E$ of the system $A$, which means $\Omega ^{(0)}_{\text{tot}}$ is constant. In symbols this can be written as</p>
<p>$$
\begin{equation}
P(E) = C \Omega^{(0)}(E),
\end{equation}\label{eq2}
$$</p>
<p>where $C$ is a constant of proportionality independent of $E$. Indeed, suppose that $A$ has an energy $E$, and $A^{\prime}$ has the corresponding energy known as to be $E^{\prime} = E^{(0)}-E$. In Eq. \eqref{eq2}, $\Omega^{(0)}(E)$ is simply given by the product</p>
<p>$$
\begin{equation}
\Omega^{(0)}(E)=\Omega(E) \Omega^{\prime}\left(E^{(0)}-E\right).
\end{equation}
$$</p>
<p>Correspondingly, system $A$ having an energy near $E$ is simply given by</p>
<p>$$
\begin{equation}
P(E)=C \Omega(E) \Omega^{\prime}\left(E^{(0)}-E\right).
\end{equation}
$$</p>
<p>To locate the position of the maximum of $P(E)$, we need to find the value when</p>
<p>$$
\begin{equation}
\frac{\partial \ln P}{\partial E}=0,
\end{equation} \label{eq5}
$$</p>
<p>where</p>
<p>$$
\ln P(E)=\ln C+\ln \Omega(E)+\ln \Omega^{\prime}\left(E^{\prime}\right).
$$</p>
<p>As what we have said before, $E^{\prime}=E^{(0)}-E$. Hence, Eq. \eqref{eq5} becomes</p>
<p>$$
\begin{equation}
\frac{\partial \ln \Omega(E)}{\partial E} - \frac{\partial \ln \Omega^{\prime}\left(E^{\prime}\right)}{\partial E^{\prime}}=0,
\end{equation}
$$</p>
<p>where we have to introduced the definition</p>
<p>$$
\begin{equation}
\beta(E) \equiv \frac{\partial \ln \Omega}{\partial E}.
\end{equation}
$$</p>
<p>Then we get $\beta(\tilde{E}) = \beta^{\prime}(\tilde{E}^{\prime})$, where $\tilde{E}$ and $\tilde{E}^{\prime}$ denote the corresponding energies of $A$ and $A^{\prime}$ at the maximum, By its definition, the parameter $\beta$ has the dimensions of a reciprocal energy. It is convenient to introduce a dimensionless parameter $T$ defined by writing $\beta = (k T)^{-1}$, where $k$ is some positive sonstant having the dimensions of energy.</p>
<p>Let me explain that why is $\beta = (k T)^{-1}$? Like what we have learnt in thermodynamics,</p>
<blockquote>
<p>entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature.</p>
</blockquote>
<p>The corresponding connection can be shown like this</p>
<p>$$
\begin{equation}
\frac{1}{T} = \frac{\partial S}{\partial E}, <br>
\end{equation}\label{eq8}
$$</p>
<p>we usually use this definition $S \equiv k \ln \Omega$, combine it with Eq. \eqref{eq8}, we get</p>
<p>$$
\begin{equation}
\frac{1}{T} = \frac{k \partial \Omega}{\partial E} \rightarrow \frac{1}{kT} \equiv \beta \equiv \frac{\partial \ln \Omega}{\partial E}. <br>
\end{equation} <br>
$$</p>
<p>The truth is that when entropy of two subsystems reaches the maximum value of the state is the most likely state for both systems.</p>
<h3 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Shannon, Claude Elwood. &ldquo;A mathematical theory of communication.&rdquo; ACM SIGMOBILE mobile computing and communications review 5.1 (2001): 3-55.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://en.wikipedia.org/wiki/Entropy">Wikipedia-Entropy</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Reif, Frederick, and Stuart A. Rice. &ldquo;Fundamentals of statistical and thermal physics.&rdquo; Physics Today 20.12 (1967): 85-87.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://kairoswang.github.io/tags/physics/">Physics</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://kairoswang.github.io/posts/2023-07-01-hofstadter_butterfly/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>ÈáèÂ≠êÂàÜÂΩ¢‰∏ñÁïå-HofstadterËù¥Ëù∂</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://kairoswang.github.io/">Love and Share</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
